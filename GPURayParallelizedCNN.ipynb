{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMgNJSjYvVWdjh/9WX2AaIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/IntroToDeepLearning/blob/main/GPURayParallelizedCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explanation of the Optimized Code with Ray on GPUs**\n",
        "\n",
        "This code applies several optimizations to improve the performance of training a neural network on the MNIST dataset using **PyTorch** and **Ray**. The optimizations include **parallel data loading**, **GPU acceleration**, and **mixed precision training**. Let's go step by step to explain each part of the code in detail.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YjNDq988iUBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Optimizations**:\n",
        "\n",
        "1. **Parallel Data Loading with Ray**:\n",
        "   - Ray is used to parallelize the data loading process. This ensures that the data is loaded concurrently across multiple workers, speeding up data preparation.\n",
        "\n",
        "2. **GPU Acceleration**:\n",
        "   - By utilizing a GPU (if available), the model can perform operations much faster than on a CPU. Moving the model and data to the GPU can result in significant speed improvements, especially for larger models and datasets.\n",
        "\n",
        "3. **Mixed Precision Training**:\n",
        "   - Mixed precision allows for faster computation and lower memory usage by using half-precision floating point (FP16) for parts of the model. It is especially useful when training on modern GPUs that support this feature.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import ray\n",
        "```\n",
        "\n",
        "- **PyTorch Libraries**:\n",
        "  - `torch`: Core PyTorch library for tensor operations.\n",
        "  - `nn`: Provides modules to build neural networks, such as `nn.Linear`.\n",
        "  - `optim`: Contains optimizers like Stochastic Gradient Descent (SGD).\n",
        "  - `F`: Contains functions like `relu` and `cross_entropy` used in forward passes.\n",
        "  - `DataLoader`: A utility to load and batch datasets.\n",
        "  - `datasets` and `transforms`: Part of `torchvision`, used for loading and transforming popular datasets like MNIST.\n",
        "\n",
        "- **Other Libraries**:\n",
        "  - `time`: Used to record training time.\n",
        "  - `ray`: A library for distributed computing, used here for parallel data loading and potentially distributing workloads.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HUfYbiQRqQ17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQQWkf8zidM6",
        "outputId": "b0f4476d-a632-4df9-8cc9-e281da67a051"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (2.35.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import ray\n"
      ],
      "metadata": {
        "id": "mkg2GGPzideJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Initialize Ray and Check Device Availability**\n",
        "\n",
        "```python\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Check if GPU is available and use it if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "```\n",
        "\n",
        "- **Ray Initialization**: `ray.init()` initializes Ray, which allows you to run tasks in parallel across multiple cores. Here, `ignore_reinit_error=True` ensures that the script continues even if Ray is already initialized.\n",
        "\n",
        "- **GPU Check**: `torch.device(\"cuda\")` checks whether a GPU is available. If a GPU is detected, the model and data are moved to the GPU for faster computations. If not, it defaults to the CPU. The `device` variable is used later to move data and models to the appropriate device.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LK6x_AIimjod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Check if GPU is available and use it if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE8anjKbmf2E",
        "outputId": "7fbf8a78-0e8e-4213-ef33-64dce915665d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-04 20:13:57,885\tINFO worker.py:1783 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. MNIST Dataset and DataLoader Setup**\n",
        "\n",
        "```python\n",
        "# Define the data transformations: Convert to tensor and normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset using torchvision\n",
        "train_dataset = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the DataLoader for batching, with num_workers for parallel data loading\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "```\n",
        "\n",
        "- **Transforms**:\n",
        "  - `ToTensor()`: Converts images from PIL format to PyTorch tensors.\n",
        "  - `Normalize((0.1307,), (0.3081,))`: Normalizes the data using the mean and standard deviation values specific to MNIST. Normalization helps the network learn faster by scaling the pixel values to a more manageable range.\n",
        "\n",
        "- **Datasets**:\n",
        "  - `datasets.MNIST`: Automatically downloads and loads the MNIST dataset. The dataset is transformed using `transform` (converted to tensors and normalized).\n",
        "\n",
        "- **DataLoader Optimizations**:\n",
        "  - `batch_size=64`: Batches the dataset into groups of 64 images, which helps optimize GPU processing.\n",
        "  - `num_workers=4`: Specifies 4 worker threads for parallel data loading. Multiple workers fetch the data in parallel to prevent the training process from waiting for data.\n",
        "  - `pin_memory=True`: This pins the memory, ensuring faster data transfer between the host (CPU) and the device (GPU).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "QVES6XAPmIL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# MNIST Dataset and Transformations\n",
        "###############################################\n",
        "\n",
        "# Define the data transformations: Convert to tensor and normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset using torchvision\n",
        "train_dataset = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the DataLoader for batching, with num_workers for parallel data loading\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "5WhRO2V3j3Rm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **4. Define the Neural Network Model**\n",
        "\n",
        "```python\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input image\n",
        "        x = F.relu(self.fc1(x))  # First hidden layer with ReLU activation\n",
        "        x = F.relu(self.fc2(x))  # Second hidden layer with ReLU activation\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n",
        "```\n",
        "\n",
        "- **Model Architecture**:\n",
        "  - A simple feed-forward neural network is defined with three layers:\n",
        "    - **Input layer**: Takes the 28x28 pixel images (MNIST) and flattens them into a vector of size 784 (`28*28`).\n",
        "    - **Hidden Layers**: Two fully connected hidden layers (`fc1` and `fc2`) with ReLU activations. The first layer reduces the dimensionality from 784 to 128 neurons, and the second reduces it further to 64 neurons.\n",
        "    - **Output Layer**: Outputs 10 values, one for each possible digit (0-9).\n",
        "  \n",
        "- **Activation Function**: `ReLU` (Rectified Linear Unit) is applied to both hidden layers to introduce non-linearity, helping the network learn more complex patterns.\n"
      ],
      "metadata": {
        "id": "cWft0zD-m7nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Define Neural Network Model\n",
        "###############################################\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input image\n",
        "        x = F.relu(self.fc1(x))  # First hidden layer with ReLU activation\n",
        "        x = F.relu(self.fc2(x))  # Second hidden layer with ReLU activation\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xNWO9grrjydj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1eQW-GgGl-2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **5. Parallel Data Loading with Ray**\n",
        "\n",
        "```python\n",
        "# Remote function to load data in parallel using Ray\n",
        "@ray.remote\n",
        "def load_batch(batch):\n",
        "    return batch\n",
        "\n",
        "# Parallelized data loading function\n",
        "def load_data_in_parallel(data_loader):\n",
        "    ray_batches = [load_batch.remote(batch) for batch in data_loader]  # Load all batches in parallel\n",
        "    return ray.get(ray_batches)  # Retrieve the loaded batches\n",
        "```\n",
        "\n",
        "- **Ray for Parallel Data Loading**:\n",
        "  - `@ray.remote`: This decorator allows functions to be run in parallel as Ray tasks. The function `load_batch` is responsible for loading individual batches of data.\n",
        "  - **Parallel Loading**: The function `load_data_in_parallel` creates a list of parallel Ray tasks (one for each batch). These tasks load batches concurrently, reducing the time it takes to prepare data for the model. After loading, `ray.get()` retrieves the results from the parallel workers.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_JjPay2UkrKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Ray-based Parallel Data Loading\n",
        "###############################################\n",
        "\n",
        "# Remote function to load data in parallel using Ray\n",
        "@ray.remote\n",
        "def load_batch(batch):\n",
        "    return batch\n",
        "\n",
        "# Parallelized data loading function\n",
        "def load_data_in_parallel(data_loader):\n",
        "    ray_batches = [load_batch.remote(batch) for batch in data_loader]  # Load all batches in parallel\n",
        "    return ray.get(ray_batches)  # Retrieve the loaded batches\n"
      ],
      "metadata": {
        "id": "J5Q3CYQRjvu3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Training Function with GPU and Mixed Precision Support**\n",
        "\n",
        "```python\n",
        "def train_model(model, optimizer, criterion, train_loader, epochs):\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Use automatic mixed precision (optional, only if supported by hardware)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # Load data in parallel using Ray\n",
        "        batches = load_data_in_parallel(train_loader)\n",
        "        \n",
        "        for data, target in batches:  # Iterate over parallel-loaded batches\n",
        "            data, target = data.to(device), target.to(device)  # Move data and target to the GPU if available\n",
        "            \n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "            \n",
        "            # Mixed precision training (optional)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(data)  # Forward pass through the network\n",
        "                loss = criterion(output, target)  # Compute loss\n",
        "            \n",
        "            scaler.scale(loss).backward()  # Backward pass to compute gradients with scaling for mixed precision\n",
        "            scaler.step(optimizer)  # Update the weights using the scaled optimizer\n",
        "            scaler.update()  # Update the scaling factor for mixed precision\n",
        "            \n",
        "            running_loss += loss.item()  # Track running loss for the epoch\n",
        "        \n",
        "        # Print the loss after each epoch\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader):.4f}')\n",
        "```\n",
        "\n",
        "- **Mixed Precision Training**:\n",
        "  - **`torch.cuda.amp.GradScaler()`**: This utility automatically scales the gradients in mixed-precision training. Mixed precision improves performance by reducing memory usage and speeding up calculations by using FP16 (16-bit floating point) where possible.\n",
        "  - **`torch.cuda.amp.autocast()`**: Automatically casts variables to lower precision where it makes sense, improving efficiency without sacrificing too much accuracy.\n",
        "\n",
        "- **Training Loop**:\n",
        "  - **Forward Pass**: The data is passed through the model to get the predicted outputs.\n",
        "  - **Loss Calculation**: The loss is computed by comparing the model's predictions with the actual labels.\n",
        "  - **Backward Pass**: The gradients of the loss are computed, and the model parameters are updated using the optimizer.\n",
        "  - **GPU Acceleration**: Both the data and the model are moved to the GPU (if available) using `data.to(device)` and `model.to(device)`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HwTD6h6skjac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Training Function with Ray-Parallelized Data Loading and GPU Support\n",
        "###############################################\n",
        "\n",
        "def train_model(model, optimizer, criterion, train_loader, epochs):\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Use automatic mixed precision (optional, only if supported by hardware)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Load data in parallel using Ray\n",
        "        batches = load_data_in_parallel(train_loader)\n",
        "\n",
        "        for data, target in batches:  # Iterate over parallel-loaded batches\n",
        "            data, target = data.to(device), target.to(device)  # Move data and target to the GPU if available\n",
        "\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "            # Mixed precision training (optional)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(data)  # Forward pass through the network\n",
        "                loss = criterion(output, target)  # Compute loss\n",
        "\n",
        "            scaler.scale(loss).backward()  # Backward pass to compute gradients with scaling for mixed precision\n",
        "            scaler.step(optimizer)  # Update the weights using the scaled optimizer\n",
        "            scaler.update()  # Update the scaling factor for mixed precision\n",
        "\n",
        "            running_loss += loss.item()  # Track running loss for the epoch\n",
        "\n",
        "        # Print the loss after each epoch\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "-PbzaZ2XjsqL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Testing Function**\n",
        "\n",
        "```python\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # No gradient computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)  # Move data and target to the GPU if available\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test\n",
        "\n",
        " Accuracy: {accuracy:.2f}%')\n",
        "```\n",
        "\n",
        "- **Evaluation Mode**: The model is set to evaluation mode (`model.eval()`), disabling features like dropout and batch normalization updates. This ensures the model behaves consistently during testing.\n",
        "- **No Gradient Calculation**: `torch.no_grad()` prevents the computation of gradients, which saves memory and speeds up inference.\n",
        "- **Accuracy Calculation**: The model’s predictions are compared to the actual labels, and accuracy is computed.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "hGswBd3Tp6AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################\n",
        "# Testing Function to Evaluate Model Performance\n",
        "###############################################\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # No gradient computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)  # Move data and target to the GPU if available\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "zR0_MVL5jmT7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Main Function**\n",
        "\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the neural network, loss function, and optimizer\n",
        "    model = Net().to(device)  # Move the model to GPU if available\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Record start time for training\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with Ray-parallelized data loading\n",
        "    train_model(model, optimizer, criterion, train_loader, epochs=5)\n",
        "\n",
        "    # Record end time and print training time\n",
        "    end_time = time.time()\n",
        "    print(f\"Training Time with Ray and Optimizations: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # Test the model on the test set\n",
        "    test_model(model, test_loader)\n",
        "\n",
        "    # Shutdown Ray\n",
        "    ray.shutdown()\n",
        "```\n",
        "\n",
        "- **Model Training**: The `train_model` function is called to train the model for 5 epochs.\n",
        "- **Timing**: The total training time is calculated using `time.time()` and printed after the training loop finishes.\n",
        "- **Model Testing**: After training, the model’s performance is evaluated on the test dataset.\n",
        "- **Ray Shutdown**: Finally, `ray.shutdown()` is called to terminate Ray once the work is done.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "B9lgGr0Hp0Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Main Function to Train and Test the Model\n",
        "###############################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the neural network, loss function, and optimizer\n",
        "    model = Net().to(device)  # Move the model to GPU if available\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Record start time for training\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with Ray-parallelized data loading\n",
        "    train_model(model, optimizer, criterion, train_loader, epochs=5)\n",
        "\n",
        "    # Record end time and print training time\n",
        "    end_time = time.time()\n",
        "    print(f\"Training Time with Ray and Optimizations: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # Test the model on the test set\n",
        "    test_model(model, test_loader)\n",
        "\n",
        "    # Shutdown Ray\n",
        "    ray.shutdown()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v8iqSlCjii2",
        "outputId": "0f3fb05f-e8c8-4620-8d3a-c6eba6bf439a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-17ff6ece3abd>:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()  # Use automatic mixed precision (optional, only if supported by hardware)\n",
            "\u001b[36m(load_batch pid=14785)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "\u001b[36m(load_batch pid=14785)\u001b[0m   return torch.load(io.BytesIO(b))\n",
            "<ipython-input-27-17ff6ece3abd>:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.8079\n",
            "Epoch 2, Training Loss: 0.3131\n",
            "Epoch 3, Training Loss: 0.2586\n",
            "Epoch 4, Training Loss: 0.2213\n",
            "Epoch 5, Training Loss: 0.1932\n",
            "Training Time with Ray and Optimizations: 40.02 seconds\n",
            "Test Accuracy: 94.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(load_batch pid=14778)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[36m(load_batch pid=14778)\u001b[0m   return torch.load(io.BytesIO(b))\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}