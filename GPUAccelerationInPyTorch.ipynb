{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPplNAZNA5SW26IEHY0mMdx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/IntroToDeepLearning/blob/main/GPUAccelerationInPyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU acceleration in PyTorch\n",
        "\n",
        "****\n",
        "\n",
        "**Summary**\n",
        "\n",
        "PyTorch provides a bridge between your high-level Python code and the immense computational power of GPUs, enabling you to train and run deep learning models with incredible speed and efficiency. It handles the complexities of memory management and hardware interaction, allowing you to focus on the core logic of your models.\n",
        "\n",
        "**1. The Hardware**\n",
        "\n",
        "* **CPU:**  The Central Processing Unit is excellent at handling a wide range of general-purpose tasks. It's designed for sequential execution and can handle complex logic and decision-making.\n",
        "* **GPU:**  The Graphical Processing Unit was originally designed for rendering graphics. However, its massively parallel architecture, with thousands of simpler cores, makes it exceptionally efficient for performing the same mathematical operations on a large amount of data simultaneously. This is ideal for the matrix and tensor calculations that underpin deep learning.\n",
        "\n",
        "**2. PyTorch's Bridge: The CUDA Backend**\n",
        "\n",
        "* **CUDA:** CUDA (Compute Unified Device Architecture) is a parallel computing platform and API developed by NVIDIA for their GPUs.\n",
        "* **PyTorch Integration:** PyTorch seamlessly connects with CUDA, enabling developers to write Python code that runs on the GPU.\n",
        "* **Key Libraries:**\n",
        "    * **cuDNN:** The CUDA Deep Neural Network library provides highly optimized implementations of core deep learning operations like convolutions, pooling, and recurrent neural networks for NVIDIA GPUs.\n",
        "    * **cuBLAS:** The CUDA Basic Linear Algebra Subprograms library accelerates fundamental linear algebra operations, including matrix multiplication, which are essential for deep learning models.\n",
        "\n",
        "**3. The Process in Motion**\n",
        "\n",
        "1. **Tensor Creation (on CPU):**\n",
        "   * When you create a tensor using `torch.tensor()` or other PyTorch tensor creation functions, the tensor is initially stored in the CPU's memory.\n",
        "\n",
        "2. **Moving to GPU (Explicit Transfer):**\n",
        "   * `tensor.to('cuda')`: This command instructs PyTorch to copy the tensor data from CPU memory to the GPU's memory.  This transfer does take time, so it's efficient to keep tensors on the GPU as long as possible during training.\n",
        "   * **Behind the Scenes:** PyTorch handles the low-level details of memory allocation and data transfer between the CPU and GPU.\n",
        "\n",
        "3. **Computation (on GPU):**\n",
        "\n",
        "   * **Automatic Dispatch:** When you perform operations on tensors located on the GPU, PyTorch's execution engine automatically dispatches those operations to the GPU for computation.\n",
        "   * **Parallel Execution:**  The GPU's many cores work in parallel to perform the calculations, taking full advantage of the tensor's structure to speed up computations dramatically.\n",
        "   * **Optimized Libraries:**  PyTorch uses cuDNN and cuBLAS to ensure these operations are executed in the most efficient way possible on the specific GPU architecture.\n",
        "\n",
        "4. **Result Retrieval (Back to CPU, if needed):**\n",
        "\n",
        "   * **tensor.cpu()**:  If you need to process the results further using CPU-bound operations or to display them, you can bring the tensor back to the CPU memory using this method.\n",
        "   * **Automatic Transfer:** In some cases, PyTorch will automatically transfer results back to the CPU if needed for certain operations.\n",
        "\n",
        "**Additional Considerations:**\n",
        "\n",
        "* **Asynchronous Execution:**  PyTorch can perform operations asynchronously, meaning it might schedule multiple GPU operations at once to further improve efficiency.\n",
        "* **Memory Management:**  PyTorch tries to optimize memory usage on both the CPU and GPU, but it's important to be mindful of memory constraints, especially when working with large models or datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zx3HY4H4SWlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. Create Tensors (on CPU by default)\n",
        "matrix_size = 1000\n",
        "x = torch.randn(matrix_size, matrix_size)\n",
        "y = torch.randn(matrix_size, matrix_size)\n",
        "\n",
        "# 2. CPU Computation\n",
        "z_cpu = torch.matmul(x, y)  # Matrix multiplication on the CPU\n",
        "\n",
        "# 3. GPU Check and Transfer\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # Get the default CUDA device\n",
        "    x_gpu = x.to(device)  # Move x to GPU memory\n",
        "    y_gpu = y.to(device)  # Move y to GPU memory\n",
        "\n",
        "    # 4. GPU Computation\n",
        "    z_gpu = torch.matmul(x_gpu, y_gpu)  # Matrix multiplication on the GPU\n",
        "\n",
        "    # 5. Bring Result Back to CPU (if needed)\n",
        "    z_cpu_from_gpu = z_gpu.cpu()  # Copy result from GPU to CPU\n",
        "\n",
        "    # Compare Results (should be very close, accounting for minor numerical differences)\n",
        "    print(torch.allclose(z_cpu, z_cpu_from_gpu))  # Output: True\n",
        "else:\n",
        "    print(\"GPU not available, computations performed on CPU only.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5Q4sksGT_so",
        "outputId": "a71ca36f-9b05-4e92-b9cf-b49942874578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Define and run the computation on-the-fly\n",
        "matrix_size = 2000  # Increase the size for a more noticeable difference\n",
        "x = torch.randn(matrix_size, matrix_size)\n",
        "y = torch.randn(matrix_size, matrix_size)\n",
        "\n",
        "# 1. CPU Computation (Time it)\n",
        "start_time = time.time()\n",
        "z_cpu = torch.matmul(x, y)\n",
        "cpu_time = time.time() - start_time\n",
        "print(f\"CPU Time: {cpu_time:.4f} seconds\")\n",
        "\n",
        "# 2. GPU Check and Transfer\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    x_gpu = x.to(device)\n",
        "    y_gpu = y.to(device)\n",
        "\n",
        "    # 3. GPU Computation (Time it)\n",
        "    start_time = time.time()\n",
        "    z_gpu = torch.matmul(x_gpu, y_gpu)\n",
        "    gpu_time = time.time() - start_time\n",
        "    print(f\"GPU Time: {gpu_time:.4f} seconds\")\n",
        "\n",
        "    # 4. Speedup Calculation\n",
        "    speedup = cpu_time / gpu_time\n",
        "    print(f\"Speedup: GPU is {speedup:.2f} times faster than CPU!\")\n",
        "\n",
        "else:\n",
        "    print(\"GPU not available, computations performed on CPU only.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gak0WQG4UcJ_",
        "outputId": "e32e7ebf-9e23-41e8-c23a-853eaf791585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Time: 0.0408 seconds\n",
            "GPU Time: 0.0006 seconds\n",
            "Speedup: GPU is 70.72 times faster than CPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cuDNN and PyTorch\n",
        "\n",
        "****\n",
        "\n",
        "**Summary**\n",
        "\n",
        "PyTorch and cuDNN form a powerful partnership that empowers developers to harness the full computational potential of NVIDIA GPUs. By intelligently integrating cuDNN's optimized implementations, PyTorch abstracts away low-level complexities, making it easier to build and train high-performance deep learning models.\n",
        "\n",
        "**cuDNN (CUDA Deep Neural Network library)**\n",
        "\n",
        "* **Optimized Primitives:** cuDNN provides highly tuned implementations for standard deep learning operations:\n",
        "    * **Convolution:** The core of many computer vision tasks, including image classification, object detection, and segmentation.\n",
        "    * **Pooling:**  Reduces dimensionality and helps extract key features from convolutional layers.\n",
        "    * **Normalization:** Ensures stability and faster convergence during training.\n",
        "    * **Activation Functions:** Introduce non-linearity to neural networks, enabling them to learn complex patterns.\n",
        "    * **Recurrent Layers:** Handle sequential data in tasks like natural language processing and time series analysis.\n",
        "    * **Matrix Multiplication:** Underlies fully connected layers and many other computations.\n",
        "\n",
        "* **Architecture-Specific Tuning:** These implementations are meticulously crafted to harness the full potential of specific NVIDIA GPU microarchitectures. This includes optimizations for:\n",
        "    * **Tensor Cores:** Specialized hardware in modern GPUs designed to accelerate matrix multiplication and convolution operations.\n",
        "    * **Memory Access Patterns:** Carefully designed data access patterns to minimize memory latency and maximize throughput.\n",
        "    * **Algorithm Optimizations:**  Employing state-of-the-art algorithms and techniques to extract the best performance from the hardware.\n",
        "\n",
        "* **Flexibility:** cuDNN offers a balance between performance and flexibility:\n",
        "    * **Graph API:** This allows defining computations as a graph of operations, enabling the runtime to schedule and optimize the execution plan for a specific neural network model.\n",
        "    * **Operator Fusion:**  cuDNN can intelligently combine multiple operations (like convolution followed by activation) into a single kernel, reducing overhead and improving performance.\n",
        "    * **Workspace:**  Provides workspace memory for intermediate calculations, allowing for further optimizations.\n",
        "\n",
        "**PyTorch's Integration**\n",
        "\n",
        "* **ATen Backend:** PyTorch's core is built upon ATen (A Tensor Library), which provides the fundamental tensor operations and handles interactions with hardware backends like CUDA.\n",
        "* **Transparent Integration:** When your PyTorch code executes on a GPU with cuDNN installed, ATen intelligently dispatches supported operations (convolutions, etc.) to cuDNN's highly optimized implementations.\n",
        "* **Automatic Selection:** PyTorch also automatically selects the most efficient cuDNN algorithms for your specific hardware configuration and tensor sizes. This can be further enhanced using the `torch.backends.cudnn.benchmark = True` flag.\n",
        "\n",
        "**Technical Deep Dive**\n",
        "\n",
        "* **Kernel Selection:**  cuDNN maintains a collection of kernels (optimized code implementations) for each supported operation. Based on factors like input tensor sizes, data types, and GPU architecture, it dynamically chooses the most efficient kernel.\n",
        "* **Memory Optimization:** cuDNN employs techniques like workspace management and memory reuse to reduce the amount of data transferred between the GPU's global memory and its faster shared memory or registers.\n",
        "* **Autotuning (Optional):** PyTorch can optionally use cuDNN's autotuning feature to find the best kernel configuration for your specific model and hardware. This can involve running multiple configurations and measuring their performance.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "* **Substantial Speedup:** By leveraging cuDNN's optimized implementations, PyTorch can achieve significant performance gains on NVIDIA GPUs for deep learning tasks.\n",
        "* **Ease of Use:** Developers can focus on writing high-level PyTorch code without worrying about the intricate low-level CUDA programming details.\n",
        "* **Framework Agnostic:** cuDNN is not tied to a specific framework. It can be used with other deep learning frameworks besides PyTorch, contributing to its widespread adoption.\n",
        "\n"
      ],
      "metadata": {
        "id": "XZdj1ICkVJ9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a Convolutional Layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "# Move to GPU (if available)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    conv_layer = conv_layer.to(device)\n",
        "\n",
        "# Input Data (on the same device as the layer)\n",
        "input_data = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "# Perform Convolution\n",
        "output = conv_layer(input_data)\n",
        "\n",
        "# PyTorch handles this:\n",
        "# 1. Detects that the input and layer are on the GPU.\n",
        "# 2. Uses cuDNN's optimized convolution implementation behind the scenes.\n",
        "# 3. Computations are performed on the GPU, significantly faster than on the CPU."
      ],
      "metadata": {
        "id": "6G2MX7n3VG3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cuBLAS: The Backbone of Linear Algebra on GPUs\n",
        "\n",
        "**cuBLAS and PyTorch**\n",
        "\n",
        "cuBLAS forms the backbone of efficient linear algebra computations on NVIDIA GPUs. By transparently integrating cuBLAS into its backend, PyTorch allows developers to effortlessly harness the power of GPUs for accelerating deep learning models without the need for explicit low-level CUDA programming.\n",
        "\n",
        "****\n",
        "\n",
        "cuBLAS (CUDA Basic Linear Algebra Subprograms) is a fundamental library developed by NVIDIA that accelerates basic linear algebra operations on NVIDIA GPUs. It provides optimized implementations of core routines for matrix and vector computations, such as:\n",
        "\n",
        "* **Matrix Multiplication (GEMM):**  The cornerstone of many deep learning models, especially for fully connected layers and transformers.\n",
        "* **Vector-Vector Operations:**  Dot products, addition, scaling, etc.\n",
        "* **Matrix-Vector Operations:**  Multiplication of a matrix and a vector.\n",
        "* **Solving Linear Systems:**  Finding solutions to systems of linear equations.\n",
        "* **Eigenvalue and Eigenvector Computations:**  Crucial for tasks like dimensionality reduction and principal component analysis.\n",
        "\n",
        "**Key Advantages of cuBLAS**\n",
        "\n",
        "* **Optimized Performance:** cuBLAS implementations are highly tuned for various NVIDIA GPU architectures, leveraging:\n",
        "    * **Tensor Cores:**  Specialized hardware for accelerating matrix operations.\n",
        "    * **Parallelism:**  Exploits the inherent parallelism of GPUs to perform many calculations simultaneously.\n",
        "    * **Memory Optimizations:**  Employs efficient data access patterns to minimize memory latency and maximize throughput.\n",
        "    * **Algorithm-Specific Tuning:**  Uses tailored algorithms for different matrix sizes and data types to achieve the best possible performance.\n",
        "\n",
        "* **Wide Range of Functionality:** cuBLAS covers an extensive set of BLAS (Basic Linear Algebra Subprograms) routines, making it a versatile tool for a variety of numerical computations.\n",
        "* **Compatibility:**  cuBLAS is designed to work seamlessly with both C and Fortran code, making it accessible to a broad range of developers.\n",
        "\n",
        "**How PyTorch Uses cuBLAS**\n",
        "\n",
        "1. **Transparent Integration:**\n",
        "   * PyTorch, through its underlying ATen library, automatically leverages cuBLAS when performing linear algebra operations on tensors located on the GPU.\n",
        "   * You typically don't need to explicitly call cuBLAS functions in your PyTorch code.\n",
        "\n",
        "2. **Efficient Dispatching:**  \n",
        "   * When PyTorch detects that an operation (like matrix multiplication) involves tensors on the GPU, it intelligently dispatches the computation to the appropriate cuBLAS routine.\n",
        "   * This ensures that the operation is executed using cuBLAS's optimized implementations, leading to significant speedups compared to CPU-based computation.\n",
        "\n",
        "3. **Automatic Selection:**  \n",
        "   * cuBLAS provides different implementations for various matrix sizes and data types.  \n",
        "   * PyTorch automatically selects the most efficient implementation based on the characteristics of your tensors and the specific GPU being used.\n",
        "\n",
        "\n",
        "**Behind the Scenes:**\n",
        "\n",
        "* PyTorch recognizes that `x` and `y` are on the GPU.\n",
        "* It utilizes cuBLAS's GEMM routine (optimized for matrix multiplication) to efficiently perform the computation on the GPU.\n",
        "* The result `result` remains on the GPU, ready for further operations or transfer back to the CPU if needed.\n",
        "\n",
        "**Significance in Deep Learning**\n",
        "\n",
        "* **Fully Connected Layers:** These layers rely heavily on matrix multiplication, making cuBLAS crucial for their efficient implementation on GPUs.\n",
        "* **Transformers:**  Transformers use self-attention mechanisms that involve extensive matrix operations, greatly benefiting from cuBLAS optimizations.\n",
        "* **Other Applications:** Any deep learning architecture utilizing linear algebra benefits from the performance gains provided by cuBLAS.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CElrZHRnXcWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "# Create Tensors (on GPU)\n",
        "device = torch.device(\"cuda\")\n",
        "x = torch.randn(1000, 500).to(device)\n",
        "y = torch.randn(500, 200).to(device)\n",
        "\n",
        "# Perform Matrix Multiplication (cuBLAS used behind the scenes)\n",
        "result = torch.matmul(x, y)\n",
        "print(\"Shape of the Tensor: \", result.shape)  # Output: torch.Size([1000, 200])\n",
        "print(\"Data Type: \", result.dtype)  # Output: torch.float32\n",
        "print(\"Type: \", result.dtype)  # Output: torch.float32\n",
        "print(\"requires_grad: \", result.requires_grad)  # Output: False\n",
        "print(\"Is Leaf: \", result.is_leaf)  # Output: False\n",
        "print(\"Device: \", result.device)  # Output: cuda:0\n",
        "print(\"Size: \", result.size())  # Output: torch.Size([1000, 200])\n",
        "print(\"Result: \", result.data)  # Output: A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxxkVNngXzTE",
        "outputId": "26b9941b-c1db-46b2-a10d-beb59d3a97e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the Tensor:  torch.Size([1000, 200])\n",
            "Data Type:  torch.float32\n",
            "Type:  torch.float32\n",
            "requires_grad:  False\n",
            "Is Leaf:  True\n",
            "Device:  cuda:0\n",
            "Size:  torch.Size([1000, 200])\n",
            "Result:  tensor([[ -5.3409, -25.0921,  31.6651,  ...,   9.7887,  19.8120,   7.1660],\n",
            "        [-19.0333,   1.3516, -42.9619,  ..., -23.1982,  -3.7588,   7.7247],\n",
            "        [-38.1926, -18.8267,  24.6628,  ...,  10.1063, -19.2557,  -2.5019],\n",
            "        ...,\n",
            "        [ 22.7634, -10.8261, -18.1449,  ...,   0.1868, -28.2732,  15.8873],\n",
            "        [  2.0095, -10.5785, -33.7464,  ...,   0.2439, -23.6063,  42.4330],\n",
            "        [-23.9185, -48.3162,   9.8585,  ..., -28.3540, -16.4420,  11.5154]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}