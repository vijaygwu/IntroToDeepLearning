{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbB47JkbqfAS5so5MYZa3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/IntroToDeepLearning/blob/main/SkipConnections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Dive into Residuals\n",
        "\n",
        "**The Vanishing Gradient Problem**\n",
        "\n",
        "Deep neural networks, while powerful, often face a challenge during training called the **vanishing gradient problem**. In backpropagation, gradients (error signals) are propagated backward through the network to update the weights. As the network gets deeper, these gradients can become increasingly small, hindering the learning process in earlier layers. This makes it difficult to train very deep networks effectively.\n",
        "\n",
        "**Residuals as a Solution**\n",
        "\n",
        "Residuals, or skip connections, are designed to address this problem. They provide a direct path for gradients to flow from later layers to earlier layers, making training deeper networks more feasible.\n",
        "\n",
        "**Mathematical Formulation**\n",
        "\n",
        "Let's break down the residual block's operation more formally:\n",
        "\n",
        "* **Input:**  `x` (the input to the block)\n",
        "* **Desired Output:**  `H(x)` (the ideal mapping the block should learn)\n",
        "* **Residual Function:** `F(x)` (the function the block actually learns)\n",
        "* **Output:** `y = F(x) + x` (the output of the residual block)\n",
        "\n",
        "The key idea is that instead of directly learning the complex mapping `H(x)`, the residual block focuses on learning the *residual* mapping `F(x)`, which is the difference between the desired output and the input:\n",
        "\n",
        "```\n",
        "F(x) = H(x) - x\n",
        "```\n",
        "\n",
        "Therefore, the desired output can be expressed as:\n",
        "\n",
        "```\n",
        "H(x) = F(x) + x\n",
        "```\n",
        "\n",
        "**Advantages of Residuals**\n",
        "\n",
        "1. **Gradient Flow:** During backpropagation, the gradient of the loss with respect to the input `x` is:\n",
        "\n",
        "```\n",
        "∂Loss/∂x = ∂Loss/∂y * (∂F(x)/∂x + 1)\n",
        "```\n",
        "\n",
        "* The `+1` term ensures that the gradient doesn't vanish even if `∂F(x)/∂x` becomes very small.\n",
        "* This allows gradients to flow more easily to earlier layers, facilitating training of deep networks.\n",
        "\n",
        "2. **Identity Mapping:**\n",
        "\n",
        "* If the residual function `F(x)` learns to output zero (or close to it), then the output `y` becomes approximately equal to the input `x`. This essentially creates an identity mapping, allowing the signal to pass through the block unchanged.\n",
        "* This is beneficial because it provides the network with the flexibility to skip layers if they don't contribute significantly to the learning process.\n",
        "\n",
        "3. **Improved Optimization:**\n",
        "\n",
        "* Residual connections can make the optimization landscape smoother, helping the optimizer converge faster and potentially find better solutions.\n",
        "\n",
        "**Assumptions**\n",
        "\n",
        "* The convolutional path represents the residual function `F(x)`.\n",
        "* The skip connection directly carries the input `x` to the end of the block.\n",
        "* The element-wise addition combines `F(x)` and `x` to produce the final output `f(x)`.\n",
        "\n",
        "**Beyond the Basics**\n",
        "\n",
        "* **Variations:** There are different types of residual blocks, such as bottleneck blocks and pre-activation blocks, each with its own advantages.\n",
        "* **Impact:** Residual connections have been instrumental in enabling the training of extremely deep neural networks, leading to breakthroughs in various domains, including image recognition and natural language processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "eSJl7NINNlSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dYvBGd1LPU5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Detailed Explanation of the Code:\n",
        "\n",
        "1. **Initialization (`__init__` method)**:\n",
        "   - **`conv1` and `conv2`**: These are the main convolutional layers in the block. The first layer might change the spatial dimensions depending on the stride, while the second layer maintains the spatial dimensions.\n",
        "   - **`bn1` and `bn2`**: Batch normalization layers are applied after each convolution to stabilize the learning process.\n",
        "   - **Skip Connection**: The skip connection allows the input to bypass the convolutional layers. If the input and output shapes differ, the skip connection adjusts the input using a 1x1 convolution.\n",
        "\n",
        "2. **Forward Pass (`forward` method)**:\n",
        "   - **Input Tensor**: The input tensor's shape is printed before any processing.\n",
        "   - **Skip Connection**: The skip connection tensor's shape is printed. This tensor is used to add to the convolutional output.\n",
        "   - **Convolutional Layers**: After each convolutional layer, the tensor's shape is printed to show how the data flows through the network.\n",
        "   - **Difference Calculation**: The difference \\( C(x) = f(x) - x \\) is calculated and printed to highlight the modification made by the convolutional layers.\n",
        "   - **Skip Connection Addition**: The skip connection is added back to the convolutional output, and the shape is printed.\n",
        "   - **Final ReLU**: The final activation function (ReLU) is applied, and the resulting shape is printed.\n",
        "\n",
        "3. **Example Usage**:\n",
        "   - **Residual Block**: An instance of the `ResidualBlock` is created with 64 input channels, 128 output channels, and a stride of 2.\n",
        "   - **Input Tensor**: A random input tensor is generated with a batch size of 32, 64 channels, and spatial dimensions of 56x56.\n",
        "   - **Forward Pass**: The input tensor is passed through the residual block, and the output shape is printed.\n",
        "   - **Loss Calculation**: A simple mean loss is computed from the output tensor.\n",
        "   - **Backward Pass**: The gradients are computed by backpropagation, and the gradients of the input tensor are printed to show how the network adjusts the input based on the loss.\n",
        "\n",
        "### Running the Code:\n",
        "When you run this code, you will see detailed print statements that explain each step of the forward pass and the effect of the skip connection on both the data flow and the gradient flow during backpropagation. This will provide a clear understanding of how residual blocks function and why skip connections are crucial for training deep networks effectively."
      ],
      "metadata": {
        "id": "58h2aSZWNl4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        # First convolutional layer:\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        # Batch normalization after the first convolution\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Second convolutional layer:\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # Batch normalization after the second convolution\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Identity shortcut (skip connection):\n",
        "        self.skip = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            # 1x1 convolution to adjust input dimensions to match output\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input Tensor Shape: \", x.shape)\n",
        "\n",
        "        # Apply the skip connection (identity mapping).\n",
        "        identity = self.skip(x)\n",
        "        print(\"Skip Connection (Identity) Shape: \", identity.shape)\n",
        "\n",
        "        # First layer: Conv1 -> BatchNorm1 -> ReLU\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        print(\"After Conv1 + BN + ReLU Shape: \", out.shape)\n",
        "\n",
        "        # Second layer: Conv2 -> BatchNorm2 (no ReLU yet)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        print(\"After Conv2 + BN Shape: \", out.shape)\n",
        "\n",
        "        # Compute the difference between the convolutional path output and the skip connection:\n",
        "        difference = out - identity\n",
        "        print(\"Difference C(x) = f(x) - x Shape: \", difference.shape)\n",
        "        print(\"Difference C(x) = f(x) - x Values (first 5 elements): \", difference.view(-1)[:5])\n",
        "\n",
        "        # Add the skip connection (identity) to the output:\n",
        "        out += identity\n",
        "        print(\"After Adding Skip Connection Shape: \", out.shape)\n",
        "\n",
        "        # Final ReLU activation applied after adding the skip connection.\n",
        "        out = F.relu(out)\n",
        "        print(\"After Final ReLU Shape: \", out.shape)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Create an instance of the ResidualBlock:\n",
        "res_block = ResidualBlock(64, 128, stride=2)\n",
        "\n",
        "# Create a random input tensor:\n",
        "input_tensor = torch.randn(32, 64, 56, 56, requires_grad=True)\n",
        "\n",
        "# Forward pass through the residual block\n",
        "output_tensor = res_block(input_tensor)\n",
        "\n",
        "# To retain the gradient of the output tensor\n",
        "output_tensor.retain_grad()\n",
        "\n",
        "print(\"Output Tensor Shape: \", output_tensor.shape)\n",
        "\n",
        "# Define a simple loss function:\n",
        "loss = output_tensor.mean()\n",
        "print(\"Loss: \", loss.item())\n",
        "\n",
        "# Perform a backward pass to compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# Print the gradient of the input tensor with respect to the loss:\n",
        "print(\"Gradient of Input Tensor (first 5 elements): \", input_tensor.grad.view(-1)[:5])\n",
        "\n",
        "# Print the gradient of the output tensor:\n",
        "print(\"Gradient of Output Tensor (first 5 elements): \", output_tensor.grad.view(-1)[:5])\n"
      ],
      "metadata": {
        "id": "My44eEs8OMzR",
        "outputId": "66019c6b-8fe1-4b40-940a-e5af9b65e467",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensor Shape:  torch.Size([32, 64, 56, 56])\n",
            "Skip Connection (Identity) Shape:  torch.Size([32, 128, 28, 28])\n",
            "After Conv1 + BN + ReLU Shape:  torch.Size([32, 128, 28, 28])\n",
            "After Conv2 + BN Shape:  torch.Size([32, 128, 28, 28])\n",
            "Difference C(x) = f(x) - x Shape:  torch.Size([32, 128, 28, 28])\n",
            "Difference C(x) = f(x) - x Values (first 5 elements):  tensor([-1.1270, -1.0176,  0.5778, -0.3388, -1.6565], grad_fn=<SliceBackward0>)\n",
            "After Adding Skip Connection Shape:  torch.Size([32, 128, 28, 28])\n",
            "After Final ReLU Shape:  torch.Size([32, 128, 28, 28])\n",
            "Output Tensor Shape:  torch.Size([32, 128, 28, 28])\n",
            "Loss:  0.5638895034790039\n",
            "Gradient of Input Tensor (first 5 elements):  tensor([ 3.6980e-09, -7.7582e-09,  8.1402e-08, -1.0858e-07,  2.1012e-07])\n",
            "Gradient of Output Tensor (first 5 elements):  tensor([3.1140e-07, 3.1140e-07, 3.1140e-07, 3.1140e-07, 3.1140e-07])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Breakdown of the Residual Block's Operation:\n",
        "\n",
        "1. **Input \\( x \\)**:\n",
        "   - This is the input tensor that is passed into the residual block. In the context of deep learning, this could be a feature map from a previous layer.\n",
        "\n",
        "2. **Desired Output \\( H(x) \\)**:\n",
        "   - This is the ideal mapping that we want the residual block to learn. For example, if we want the network to extract a certain feature from the input, \\( H(x) \\) represents this feature extraction.\n",
        "\n",
        "3. **Residual Function \\( F(x) \\)**:\n",
        "   - Instead of directly learning \\( H(x) \\), the residual block is designed to learn the residual function \\( F(x) = H(x) - x \\). The idea is that learning the difference (or residual) between the input and the desired output is often easier than learning the entire function \\( H(x) \\) directly.\n",
        "\n",
        "4. **Output \\( y \\)**:\n",
        "   - The output of the residual block is \\( y = F(x) + x \\). This is the result of adding the residual function \\( F(x) \\) to the original input \\( x \\), which effectively gives us \\( H(x) \\).\n",
        "\n",
        "### Explanation in the Context of the Code:\n",
        "\n",
        "Let’s relate this to the code:\n",
        "\n",
        "- **Input \\( x \\)**: The input to the block is the tensor `input_tensor`, which could represent feature maps with shape `[batch_size, channels, height, width]`.\n",
        "\n",
        "- **Desired Output \\( H(x) \\)**: In theory, this is what we want the block to learn. However, the block doesn’t directly learn \\( H(x) \\); it learns \\( F(x) \\).\n",
        "\n",
        "- **Residual Function \\( F(x) \\)**:\n",
        "  - In the code, \\( F(x) \\) is computed through two convolutional layers with batch normalization and ReLU activation:\n",
        "    ```python\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    ```\n",
        "  - This represents the residual function \\( F(x) \\).\n",
        "\n",
        "- **Output \\( y = F(x) + x \\)**:\n",
        "  - After computing \\( F(x) \\), the code adds the original input \\( x \\) (or `identity`, which might be a transformed version of \\( x \\)) to the output of the residual function:\n",
        "    ```python\n",
        "    out += identity\n",
        "    out = F.relu(out)\n",
        "    ```\n",
        "  - This addition is where the skip connection happens, resulting in the output \\( y = F(x) + x \\). The final ReLU activation is applied to introduce non-linearity.\n",
        "\n",
        "### Implementation in the Code:\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "    # x is the input to the block\n",
        "    print(\"Input Tensor Shape: \", x.shape)\n",
        "\n",
        "    # Skip connection (identity mapping)\n",
        "    identity = self.skip(x)\n",
        "    print(\"Skip Connection (Identity) Shape: \", identity.shape)\n",
        "    \n",
        "    # Compute the residual function F(x)\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    print(\"Residual Function F(x) Shape: \", out.shape)\n",
        "    \n",
        "    # Add the skip connection: y = F(x) + x\n",
        "    out += identity\n",
        "    print(\"After Adding Skip Connection: y = F(x) + x Shape: \", out.shape)\n",
        "    \n",
        "    # Final ReLU activation\n",
        "    out = F.relu(out)\n",
        "    print(\"Final Output After ReLU Shape: \", out.shape)\n",
        "    \n",
        "    return out\n",
        "```\n",
        "\n",
        "### Example Workflow:\n",
        "\n",
        "1. **Input Tensor \\( x \\)**: Suppose the input tensor `input_tensor` has a shape of `[32, 64, 56, 56]`, representing a batch of 32 images with 64 channels and spatial dimensions 56x56.\n",
        "\n",
        "2. **Skip Connection \\( x \\)**: The skip connection (`identity`) is either the original input or a transformed version (e.g., passed through a 1x1 convolution if dimensions don’t match).\n",
        "\n",
        "3. **Residual Function \\( F(x) \\)**: The two convolutional layers transform \\( x \\) into a new tensor `out`, representing the residual function \\( F(x) \\).\n",
        "\n",
        "4. **Output \\( y = F(x) + x \\)**: The output tensor is computed by adding the residual function \\( F(x) \\) to the skip connection (`identity`). This step effectively \"skips\" the convolutions if they don't contribute much to learning, which helps in training deep networks by avoiding the vanishing gradient problem.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- The residual block simplifies the learning task by having the network focus on learning the residual function \\( F(x) = H(x) - x \\) rather than directly learning \\( H(x) \\).\n",
        "- The output of the residual block, \\( y = F(x) + x \\), combines the learned residual with the original input, which allows the network to easily preserve useful information from the input."
      ],
      "metadata": {
        "id": "MntEBShAPWim"
      }
    }
  ]
}