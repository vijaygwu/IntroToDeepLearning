{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPKHaswZz+Pui+11ilorvSg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/IntroToDeepLearning/blob/main/CNNwithAndWithoutPIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Explanation of the Code(without a CNN)**\n",
        "\n",
        "This code is designed to compare two approaches for loading and processing the MNIST dataset in PyTorch: one using the `PIL` library for image handling and the other directly using PyTorch's built-in tensor handling through `torchvision.datasets.MNIST`. Both approaches involve training a simple neural network to classify handwritten digits from the MNIST dataset, and the results are compared in terms of training time and test accuracy.\n",
        "\n",
        "Let's go through the code step by step.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "```\n",
        "\n",
        "- **`torch`**: Core PyTorch library.\n",
        "- **`torch.nn`**: Provides modules to build neural networks.\n",
        "- **`torch.optim`**: Contains optimizers like SGD, Adam, etc.\n",
        "- **`torch.nn.functional`**: Contains functions like `relu` and `cross_entropy` that are commonly used in neural networks.\n",
        "- **`torch.utils.data.DataLoader`**: Loads datasets in batches during training.\n",
        "- **`torchvision.datasets`**: Provides popular datasets like MNIST.\n",
        "- **`torchvision.transforms`**: Contains functions to transform data, such as converting images to tensors and normalizing them.\n",
        "- **`PIL`**: Used for handling image files manually.\n",
        "- **`time`**: For tracking execution time of training loops.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dst78QL6AKrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d5BFviQPIgYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import time\n"
      ],
      "metadata": {
        "id": "Dg7DU2doALVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **2. Version 1: Using PIL for Image Handling**\n",
        "\n",
        "#### Custom Dataset Class\n",
        "\n",
        "```python\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, mnist_data, transform=None):\n",
        "        self.mnist_data = mnist_data  # Store the MNIST dataset from torchvision.\n",
        "        self.transform = transform    # Transformation like converting to tensor and normalization.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)  # Return the number of images in the dataset.\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.mnist_data[index]  # Get an image and its label.\n",
        "        img = transforms.ToPILImage()(img)   # Convert the tensor image back to a PIL image.\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)  # Apply the specified transformations (to tensor, normalization).\n",
        "\n",
        "        return img, label  # Return the transformed image and its label.\n",
        "```\n",
        "\n",
        "- **Purpose**: This class wraps the `torchvision.datasets.MNIST` dataset and allows the manual conversion of images from tensors back to `PIL` images for custom processing. It also applies transformations like converting the image back to a tensor and normalizing it.\n",
        "  \n",
        "#### Dataset Loading and Transformations\n",
        "\n",
        "```python\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the PIL image to a tensor.\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the data with the mean and std of MNIST.\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_dataset_pil = MNISTDataset(train_data, transform=transform)\n",
        "test_dataset_pil = MNISTDataset(test_data, transform=transform)\n",
        "\n",
        "train_loader_pil = DataLoader(dataset=train_dataset_pil, batch_size=64, shuffle=True)\n",
        "test_loader_pil = DataLoader(dataset=test_dataset_pil, batch_size=64, shuffle=False)\n",
        "```\n",
        "\n",
        "- **Transformations**:\n",
        "  - `ToTensor()`: Converts the PIL image to a PyTorch tensor.\n",
        "  - `Normalize((0.1307,), (0.3081,))`: Standard normalization for the MNIST dataset (mean and std are specific to MNIST).\n",
        "  \n",
        "- **Datasets**:\n",
        "  - `datasets.MNIST`: Automatically downloads and loads the MNIST dataset if it's not already available.\n",
        "  \n",
        "- **DataLoader**: The `DataLoader` is used to load the data in batches, with `shuffle=True` for the training data to randomize the order of images for better generalization.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vIItfry4HKRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Version 1: Using PIL                        #\n",
        "###############################################\n",
        "\n",
        "# Custom Dataset Class for MNIST using PIL and torchvision\n",
        "# This class wraps the torchvision MNIST dataset but loads images using PIL to allow for manual control over image processing.\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, mnist_data, transform=None):\n",
        "        self.mnist_data = mnist_data  # Store the dataset passed in (torchvision MNIST dataset).\n",
        "        self.transform = transform    # Transformation (like converting to tensors and normalizing).\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)  # Return the number of items in the dataset.\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get the image and label at the specified index from the original dataset.\n",
        "        img, label = self.mnist_data[index]\n",
        "\n",
        "        # Convert the image from a tensor back to a PIL image for further processing.\n",
        "        img = transforms.ToPILImage()(img)\n",
        "\n",
        "        # Apply any transformations (like converting back to a tensor and normalizing).\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Return the processed image and its corresponding label.\n",
        "        return img, label\n",
        "\n",
        "# Set up transforms (convert to tensor and normalize)\n",
        "# We need to convert the images to tensors and normalize them (mean and std values are specific to MNIST).\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor.\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the data with MNIST-specific mean and std.\n",
        "])\n",
        "\n",
        "# Download the MNIST dataset using torchvision.\n",
        "# The dataset will be downloaded if not already present in './mnist_data'.\n",
        "train_data = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Wrap the torchvision MNIST dataset with our custom dataset class, which uses PIL for image handling.\n",
        "train_dataset_pil = MNISTDataset(train_data, transform=transform)\n",
        "test_dataset_pil = MNISTDataset(test_data, transform=transform)\n",
        "\n",
        "# DataLoader for batching. Batching helps in loading a set of images at once during training.\n",
        "# Shuffle=True ensures that the training data is shuffled each epoch for better generalization.\n",
        "train_loader_pil = DataLoader(dataset=train_dataset_pil, batch_size=64, shuffle=True)\n",
        "test_loader_pil = DataLoader(dataset=test_dataset_pil, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CXDyQi5IFLm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3. Version 2: Without PIL for Image Handling**\n",
        "\n",
        "```python\n",
        "train_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)\n",
        "test_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader_no_pil = DataLoader(dataset=train_dataset_no_pil, batch_size=64, shuffle=True)\n",
        "test_loader_no_pil = DataLoader(dataset=test_dataset_no_pil, batch_size=64, shuffle=False)\n",
        "```\n",
        "\n",
        "- **Difference**: In this version, the MNIST dataset is directly handled by `torchvision.datasets.MNIST`. The images are loaded as tensors right from the start, so there's no need for manual conversion using `PIL`.\n",
        "\n",
        "- **Advantages**: This is more efficient when working with standard datasets like MNIST because the data is already prepared in tensor format.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6KzunubjHBpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Version 2: Without PIL                      #\n",
        "###############################################\n",
        "\n",
        "# In this version, we use the dataset directly as provided by torchvision, without wrapping it in a custom dataset class.\n",
        "\n",
        "# Transformations (convert to tensor and normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Directly convert the images to PyTorch tensors.\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the data (mean and std specific to MNIST).\n",
        "])\n",
        "\n",
        "# Download and load the MNIST dataset directly.\n",
        "# The dataset will be downloaded and directly loaded without any manual PIL processing.\n",
        "train_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)\n",
        "test_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader for batching. Same as the PIL version.\n",
        "train_loader_no_pil = DataLoader(dataset=train_dataset_no_pil, batch_size=64, shuffle=True)\n",
        "test_loader_no_pil = DataLoader(dataset=test_dataset_no_pil, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "eIoDcnaXDvAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Neural Network Architecture**\n",
        "\n",
        "```python\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # Fully connected layer 1: from input size (28*28) to 128 neurons.\n",
        "        self.fc2 = nn.Linear(128, 64)     # Fully connected layer 2: from 128 neurons to 64.\n",
        "        self.fc3 = nn.Linear(64, 10)      # Output layer: 10 neurons for 10 digit classes.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the 28x28 image into a vector of size 784.\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first layer.\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation to the second layer.\n",
        "        x = self.fc3(x)          # No activation here (cross-entropy will handle softmax).\n",
        "        return x\n",
        "```\n",
        "\n",
        "- **Network Overview**:\n",
        "  - Input size is `28x28` (since MNIST images are 28x28 pixels).\n",
        "  - Two hidden layers with ReLU activation.\n",
        "  - The final output layer has 10 neurons (one for each digit class).\n",
        "\n",
        "- **Purpose**: The network takes in a flattened image, processes it through two fully connected layers with ReLU activation, and then outputs a vector of 10 scores (one for each digit).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QFEJeT__Gzwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Shared Neural Network Code                  #\n",
        "###############################################\n",
        "\n",
        "# Define a simple fully connected neural network for classification.\n",
        "# The model has three layers: two hidden layers with ReLU activation and one output layer for classification.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input size is 28*28 (since MNIST images are 28x28 pixels).\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # First fully connected layer, 128 neurons.\n",
        "        self.fc2 = nn.Linear(128, 64)     # Second fully connected layer, 64 neurons.\n",
        "        self.fc3 = nn.Linear(64, 10)      # Output layer, 10 neurons (for 10 digit classes).\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor (28x28 pixels) into a vector of size 784.\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first layer.\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation to the second layer.\n",
        "        x = self.fc3(x)          # Output layer (no activation, as we'll use CrossEntropyLoss).\n",
        "        return x\n",
        "\n",
        "# Create separate models for the two versions (PIL and No PIL).\n",
        "model_pil = Net()      # Model for the PIL version.\n",
        "model_no_pil = Net()   # Model for the non-PIL version."
      ],
      "metadata": {
        "id": "r76Q_b_xGvaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **5. Loss Function and Optimizer**\n",
        "\n",
        "```python\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification tasks.\n",
        "optimizer_pil = optim.SGD(model_pil.parameters(), lr=0.01)  # Optimizer for the PIL version.\n",
        "optimizer_no_pil = optim.SGD(model_no_pil.parameters(), lr=0.01)  # Optimizer for the non-PIL version.\n",
        "```\n",
        "\n",
        "- **`CrossEntropyLoss`**: This loss function is used for classification tasks. It combines `LogSoftmax` and `Negative Log Likelihood` in one function.\n",
        "- **`SGD` Optimizer**: Stochastic Gradient Descent is used for optimization. The learning rate is set to 0.01 for both models.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "T3R_viJhGtaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Use CrossEntropyLoss for classification tasks and SGD for optimization.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_pil = optim.SGD(model_pil.parameters(), lr=0.01)  # Optimizer for the PIL version.\n",
        "optimizer_no_pil = optim.SGD(model_no_pil.parameters(), lr=0.01)  # Optimizer for the non-PIL version.\n"
      ],
      "metadata": {
        "id": "eSXqt4uHD-n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Training and Testing Loop for PIL Version**\n",
        "\n",
        "#### Training Loop\n",
        "\n",
        "```python\n",
        "start_time_pil = time.time()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model_pil.train()  # Set the model to training mode.\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader_pil):\n",
        "        optimizer_pil.zero_grad()  # Clear previous gradients.\n",
        "        output = model_pil(data)  # Forward pass through the network.\n",
        "        loss = criterion(output, target)  # Compute the loss.\n",
        "        loss.backward()  # Backward pass to compute gradients.\n",
        "        optimizer_pil.step()  # Update model weights.\n",
        "        running_loss += loss.item()  # Accumulate the loss.\n",
        "    \n",
        "    print(f'PIL Version - Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader_pil):.4f}')\n",
        "```\n",
        "\n",
        "- **Training Process**:\n",
        "  - **Forward pass**: The data is passed through the network to make predictions.\n",
        "  - **Loss calculation**: The difference between the predicted output and true labels is computed using cross-entropy loss.\n",
        "  - **Backward pass**: The gradients of the loss with respect to the model parameters are computed.\n",
        "  - **Optimization**: The optimizer updates the model parameters based on the gradients.\n",
        "\n",
        "#### Testing Loop\n",
        "\n",
        "```python\n",
        "model_pil.eval()  # Set the model to evaluation mode (no gradient calculation).\n",
        "correct_pil = 0\n",
        "total_pil = 0\n",
        "\n",
        "with torch.no_grad():  # No need to compute gradients during evaluation.\n",
        "    for data, target in test_loader_pil:\n",
        "        outputs = model_pil(data)  # Forward pass.\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the predicted class.\n",
        "        total_pil += target.size(0)  # Increment the total number of samples.\n",
        "        correct_pil += (predicted == target).sum().item()  # Count correct predictions.\n",
        "\n",
        "accuracy_pil = 100 * correct_pil / total_pil  # Compute accuracy.\n",
        "print(f'PIL Version - Test Accuracy: {accuracy_pil:.2f}%')\n",
        "```\n",
        "\n",
        "- **Evaluation**: The model is evaluated on the test set by making\n",
        "\n",
        " predictions, comparing them to the true labels, and calculating accuracy.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "oootkHEeGc9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################\n",
        "# Training and Testing for PIL Version        #\n",
        "###############################################\n",
        "\n",
        "# Record start time for the PIL version to measure training time.\n",
        "start_time_pil = time.time()\n",
        "\n",
        "# Training loop for PIL version\n",
        "for epoch in range(5):  # We train the model for 5 epochs.\n",
        "    model_pil.train()  # Set the model to training mode.\n",
        "    running_loss = 0.0  # Variable to track loss over the epoch.\n",
        "\n",
        "    # Loop over batches of data in the training set.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader_pil):\n",
        "        optimizer_pil.zero_grad()  # Zero the gradients (required before every backward pass).\n",
        "        output = model_pil(data)   # Forward pass: get predictions from the model.\n",
        "        loss = criterion(output, target)  # Calculate the loss (how far predictions are from true labels).\n",
        "        loss.backward()  # Backward pass: compute gradients.\n",
        "        optimizer_pil.step()  # Update model weights based on gradients.\n",
        "        running_loss += loss.item()  # Accumulate the loss.\n",
        "\n",
        "    # Print the average loss for the epoch.\n",
        "    print(f'PIL Version - Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader_pil):.4f}')\n",
        "\n",
        "# Testing the model for PIL version\n",
        "model_pil.eval()  # Set the model to evaluation mode (no backpropagation, etc.).\n",
        "correct_pil = 0   # To count how many predictions were correct.\n",
        "total_pil = 0     # To count the total number of examples.\n",
        "\n",
        "# Loop through the test dataset.\n",
        "with torch.no_grad():  # No need to compute gradients during evaluation.\n",
        "    for data, target in test_loader_pil:\n",
        "        outputs = model_pil(data)  # Forward pass: get predictions.\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the highest score as the prediction.\n",
        "        total_pil += target.size(0)  # Increment the total number of examples.\n",
        "        correct_pil += (predicted == target).sum().item()  # Count correct predictions.\n",
        "\n",
        "# Record end time for the PIL version.\n",
        "end_time_pil = time.time()\n",
        "training_time_pil = end_time_pil - start_time_pil  # Calculate total training time.\n",
        "accuracy_pil = 100 * correct_pil / total_pil  # Calculate accuracy as a percentage.\n",
        "\n",
        "print(f'PIL Version - Test Accuracy: {accuracy_pil:.2f}%')\n",
        "print(f'PIL Version - Training Time: {training_time_pil:.2f} seconds')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd72OT5sED68",
        "outputId": "ab757867-257f-4c37-93a0-c97f5b810fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIL Version - Epoch 1, Training Loss: 0.8008\n",
            "PIL Version - Epoch 2, Training Loss: 0.3135\n",
            "PIL Version - Epoch 3, Training Loss: 0.2565\n",
            "PIL Version - Epoch 4, Training Loss: 0.2187\n",
            "PIL Version - Epoch 5, Training Loss: 0.1904\n",
            "PIL Version - Test Accuracy: 94.78%\n",
            "PIL Version - Training Time: 117.36 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################\n",
        "# Training and Testing for No PIL Version     #\n",
        "###############################################\n",
        "\n",
        "# Record start time for the non-PIL version.\n",
        "start_time_no_pil = time.time()\n",
        "\n",
        "# Training loop for no PIL version (same as PIL version, but using the non-PIL data loader).\n",
        "for epoch in range(5):\n",
        "    model_no_pil.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Loop over batches of data in the training set.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader_no_pil):\n",
        "        optimizer_no_pil.zero_grad()  # Zero the gradients.\n",
        "        output = model_no_pil(data)   # Forward pass.\n",
        "        loss = criterion(output, target)  # Calculate the loss.\n",
        "        loss.backward()  # Backward pass: compute gradients.\n",
        "        optimizer_no_pil.step()  # Update model weights.\n",
        "        running_loss += loss.item()  # Accumulate the loss.\n",
        "\n",
        "    # Print the average loss for the epoch.\n",
        "    print(f'No PIL Version - Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader_no_pil):.4f}')\n",
        "\n",
        "# Testing the model for no PIL version\n",
        "model_no_pil.eval()  # Set the model to evaluation mode.\n",
        "correct_no_pil = 0   # To count how many predictions were correct.\n",
        "total_no_pil = 0     # To count the total number of examples.\n",
        "\n",
        "# Loop through the test dataset.\n",
        "with torch.no_grad():  # No need to compute gradients during evaluation.\n",
        "    for data, target in test_loader_no_pil:\n",
        "        outputs = model_no_pil(data)  # Forward pass.\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the highest score as the prediction.\n",
        "        total_no_pil += target.size(0)  # Increment the total number of examples.\n",
        "        correct_no_pil += (predicted == target).sum().item()  # Count correct predictions.\n",
        "\n",
        "# Record end time for the non-PIL version.\n",
        "end_time_no_pil = time.time()\n",
        "training_time_no_pil = end_time_no_pil - start_time_no_pil  # Calculate total training time.\n",
        "accuracy_no_pil = 100 * correct_no_pil / total_no_pil  # Calculate accuracy as a percentage.\n",
        "\n",
        "print(f'No PIL Version - Test Accuracy: {accuracy_no_pil:.2f}%')\n",
        "print(f'No PIL Version - Training Time: {training_time_no_pil:.2f} seconds')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR8O3x2aEJBU",
        "outputId": "0c0313de-72ab-4671-e361-356b95b94d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No PIL Version - Epoch 1, Training Loss: 0.8548\n",
            "No PIL Version - Epoch 2, Training Loss: 0.3192\n",
            "No PIL Version - Epoch 3, Training Loss: 0.2609\n",
            "No PIL Version - Epoch 4, Training Loss: 0.2233\n",
            "No PIL Version - Epoch 5, Training Loss: 0.1952\n",
            "No PIL Version - Test Accuracy: 94.50%\n",
            "No PIL Version - Training Time: 68.90 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Timing and Comparison**\n",
        "\n",
        "```python\n",
        "# Timing and accuracy are tracked for both versions.\n",
        "end_time_pil = time.time()\n",
        "training_time_pil = end_time_pil - start_time_pil\n",
        "\n",
        "print(f'PIL Version - Training Time: {training_time_pil:.2f} seconds')\n",
        "\n",
        "# Repeat the same for the non-PIL version.\n",
        "```\n",
        "\n",
        "- **Timing**: The `time.time()` function is used to measure how long it takes to train the model for both versions. This allows a direct comparison of training times.\n",
        "\n",
        "- **Results Comparison**:\n",
        "  - Training times and accuracies for both the `PIL` and non-`PIL` versions are printed side by side to compare the performance of each approach.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fOwGUmksGNe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Results Comparison                          #\n",
        "###############################################\n",
        "\n",
        "# Print a side-by-side comparison of the training times and test accuracies.\n",
        "print(\"\\n================= Comparison =================\")\n",
        "print(f\"Training Time (PIL): {training_time_pil:.2f} seconds\")\n",
        "print(f\"Training Time (No PIL): {training_time_no_pil:.2f} seconds\")\n",
        "print(f\"Test Accuracy (PIL): {accuracy_pil:.2f}%\")\n",
        "print(f\"Test Accuracy (No PIL): {accuracy_no_pil:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0XeINOWELBt",
        "outputId": "7fbf0c2b-b6bb-4aa0-8b6f-ad6d68884695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================= Comparison =================\n",
            "Training Time (PIL): 117.36 seconds\n",
            "Training Time (No PIL): 68.90 seconds\n",
            "Test Accuracy (PIL): 94.78%\n",
            "Test Accuracy (No PIL): 94.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using PIL and not using PIL**\n",
        "\n",
        "| **Aspect**                  | **Using PIL**                                      | **Without PIL**                                     |\n",
        "|-----------------------------|----------------------------------------------------|----------------------------------------------------|\n",
        "| **Image Handling**           | Converts image tensors back to **PIL** images. This allows for custom image processing using `PIL` (Python Imaging Library). | Directly uses tensors from the dataset without converting to PIL. This avoids the overhead of image format conversions. |\n",
        "| **Custom Dataset Class**     | Requires a custom `MNISTDataset` class to load images using `PIL` and apply transformations manually. | Does not require a custom dataset class. The `torchvision.datasets.MNIST` dataset is directly used as tensors. |\n",
        "| **Transformations**          | The images are first converted back to **PIL** images and then transformed back to tensors using `transforms.ToTensor()`. This allows for more flexibility with custom image handling if needed. | Transformations (e.g., `ToTensor()` and normalization) are applied directly to the image tensors using `torchvision.transforms`. No need for PIL-based image transformations. |\n",
        "| **Efficiency**               | **Less efficient**: Converting images from tensors to **PIL** and then back to tensors introduces overhead, making this approach slower, especially for large datasets. | **More efficient**: Directly working with tensors avoids unnecessary conversions, making it faster and more suitable for large-scale datasets like MNIST. |\n",
        "| **Flexibility**              | **More flexible**: If custom image processing (e.g., resizing, cropping, augmentations) is required, using `PIL` allows for advanced image manipulation that isn't always available in `torchvision.transforms`. | **Less flexible**: `torchvision.transforms` is powerful for common image processing needs, but it might not cover advanced or specific custom operations that **PIL** can handle. |\n",
        "| **Code Complexity**          | **Higher complexity**: Requires a custom dataset class to manage PIL conversions and manual handling of transformations. This adds extra code and complexity. | **Lower complexity**: Simply using `torchvision.datasets.MNIST` directly with transformations reduces code complexity, making it easier to implement and maintain. |\n",
        "| **Use Case Suitability**     | Suitable if you need **custom image preprocessing** or manipulation (e.g., resizing, filtering, augmentation) before converting to tensors. Common in projects requiring advanced preprocessing beyond normalization or conversion. | Suitable for most standard datasets where the focus is on efficient loading and training. Common in projects where you need fast, **out-of-the-box dataset handling**, especially for widely used datasets like MNIST. |\n",
        "| **Training Time**            | Takes longer due to the additional conversion steps between tensor and PIL images. This extra step increases the overall training time, especially noticeable with large datasets or high epochs. | Faster since the images are handled as tensors directly. Avoiding the PIL conversion reduces unnecessary overhead, improving training time. |\n",
        "| **Code Maintenance**         | More difficult to maintain, especially if adding or modifying the transformations requires working through a custom dataset class. | Easier to maintain since you rely on PyTorch's well-documented and widely-used data handling functionality. |\n",
        "| **Memory Overhead**          | Higher memory usage since each image is converted between formats, which can be taxing when working with large datasets. | Lower memory overhead since the data remains in tensor format, which is native to PyTorch and more memory efficient. |\n",
        "| **Transform Customizability** | Provides full control over how images are loaded, processed, and transformed. You can create custom pipelines involving PIL methods before converting to a tensor. | Less customizable but still allows common transformations like normalization, resizing, and data augmentation with `torchvision.transforms`. Custom transformations can still be added but in a more restricted environment. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points:**\n",
        "\n",
        "1. **Performance**:\n",
        "   - The **without PIL** approach is faster and more efficient because it skips the unnecessary step of converting between image formats. By directly handling the images as tensors, this method allows for faster data loading, training, and testing, particularly important in large-scale datasets.\n",
        "\n",
        "2. **Flexibility**:\n",
        "   - The **using PIL** approach offers more flexibility for custom image manipulation. For instance, if you need to perform advanced image preprocessing, like applying filters, specific augmentations, or detailed custom transformations, the PIL approach gives you more control.\n",
        "   - However, **without PIL** is still capable of common transformations like resizing, normalization, and flipping, but it's more constrained to the functionalities provided by `torchvision.transforms`.\n",
        "\n",
        "3. **Complexity**:\n",
        "   - **Using PIL** adds complexity because it requires creating a custom dataset class and manually handling image conversions. This additional code increases the risk of bugs and makes the code more difficult to maintain.\n",
        "   - **Without PIL** is simpler and easier to manage since you're using PyTorch’s built-in functions for handling datasets and transformations.\n",
        "\n",
        "4. **Use Cases**:\n",
        "   - **Using PIL** is more appropriate when working with custom datasets where you might need non-standard image preprocessing steps.\n",
        "   - **Without PIL** is ideal for standard tasks like MNIST classification, where the dataset is already structured and doesn't require complex image manipulations. This approach is faster and easier to implement.\n",
        "\n",
        "---\n",
        "\n",
        "### **Which Approach Should You Use?**\n",
        "\n",
        "- **Use `PIL`** when:\n",
        "  - You need **advanced image preprocessing**.\n",
        "  - You're working with **custom datasets** that require custom image handling.\n",
        "  - You want **fine-grained control** over how images are loaded and processed.\n",
        "\n",
        "- **Skip `PIL` (Use tensors directly)** when:\n",
        "  - You're working with **standard datasets** like MNIST, CIFAR, etc.\n",
        "  - You prioritize **efficiency** and **simplicity**.\n",
        "  - You want to reduce **code complexity** and **training time**.\n",
        "\n",
        "In conclusion, for most typical scenarios like MNIST classification, **not using PIL** is the better choice due to its efficiency, simplicity, and ease of use. However, **using PIL** offers more control when you need custom processing for complex datasets."
      ],
      "metadata": {
        "id": "N3EQbH6_IpeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **For more information**\n",
        "\n",
        "1. **PyTorch Documentation**:\n",
        "   - [PyTorch Datasets and DataLoader](https://pytorch.org/docs/stable/data.html)\n",
        "     - The DataLoader and Dataset classes are foundational to PyTorch’s data handling mechanisms. They support efficient batching, shuffling, and loading of data.\n",
        "   - [torchvision.datasets.MNIST](https://pytorch.org/vision/stable/datasets.html#mnist)\n",
        "     - torchvision's dataset handling, specifically for MNIST, which is a widely used dataset for digit classification tasks.\n",
        "   - [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)\n",
        "     - PyTorch provides standard image transformations such as converting to tensors and normalizing image data. This is the foundation of the **without PIL** approach.\n",
        "\n",
        "2. **Python Imaging Library (PIL/Pillow)**:\n",
        "   - [Pillow Documentation](https://pillow.readthedocs.io/en/stable/)\n",
        "     - PIL (now maintained as Pillow) is a Python library for image processing. It offers functionality to open, manipulate, and save images in various formats. The **using PIL** approach leverages these capabilities for custom image handling.\n",
        "\n",
        "3. **PyTorch’s Autograd and Optimization**:\n",
        "   - [PyTorch Autograd Documentation](https://pytorch.org/docs/stable/autograd.html)\n",
        "     - The process of automatic differentiation and optimization using backward propagation is explained in the PyTorch Autograd docs. This is crucial to understanding how gradients are computed and applied to update model parameters during training.\n"
      ],
      "metadata": {
        "id": "FF_AB4w6RXWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code with a CNN**\n",
        "\n",
        "**Explanation of the code**\n",
        "\n",
        "1. Imports and Setup:\n",
        "   The code imports necessary PyTorch libraries and other utilities.\n",
        "\n",
        "2. CNN Architecture (class Net):\n",
        "   - This class defines the CNN structure.\n",
        "   - conv1: First convolutional layer (1 input channel, 32 output channels, 3x3 kernel)\n",
        "   - conv2: Second convolutional layer (32 input channels, 64 output channels, 3x3 kernel)\n",
        "   - dropout1 and dropout2: Dropout layers to prevent overfitting\n",
        "   - fc1 and fc2: Fully connected layers for final classification\n",
        "\n",
        "   The forward method defines how data flows through the network:\n",
        "   - Apply convolutions with ReLU activation\n",
        "   - Max pooling to reduce spatial dimensions\n",
        "   - Dropout for regularization\n",
        "   - Flatten the output and pass through fully connected layers\n",
        "   - Apply log softmax for classification output\n",
        "\n",
        "3. Data Loading (PIL version):\n",
        "   - Custom MNISTDataset class for handling PIL image conversions\n",
        "   - Data transformations (ToTensor and Normalize)\n",
        "   - Loading MNIST dataset and wrapping with custom dataset class\n",
        "   - Creating DataLoader for batching\n",
        "\n",
        "4. Data Loading (Non-PIL version):\n",
        "   - Directly uses torchvision's MNIST dataset\n",
        "   - Same transformations as PIL version\n",
        "   - Creating DataLoader for batching\n",
        "\n",
        "5. Training Function:\n",
        "   - Iterates over batches in the training data\n",
        "   - Moves data to the appropriate device (CPU/GPU)\n",
        "   - Performs forward pass, calculates loss, backpropagation, and optimization\n",
        "   - Prints training progress\n",
        "\n",
        "6. Testing Function:\n",
        "   - Evaluates the model on the test set\n",
        "   - Calculates average loss and accuracy\n",
        "   - Returns the accuracy for comparison\n",
        "\n",
        "7. Training and Testing Loop:\n",
        "   - Sets up models, optimizers, and device (CPU/GPU)\n",
        "   - Trains and tests both PIL and non-PIL versions\n",
        "   - Measures training time for each version\n",
        "\n",
        "8. Results Comparison:\n",
        "   - Prints training times and test accuracies for both versions\n",
        "\n",
        "Key Differences from Previous Code:\n",
        "- Uses a CNN architecture instead of a simple fully connected network\n",
        "- Implements separate train and test functions\n",
        "- Uses Adam optimizer instead of SGD\n",
        "- Supports GPU training if available\n",
        "- Uses NLL Loss with log softmax output\n",
        "\n",
        "This CNN should perform better on image classification tasks like MNIST compared to the fully connected network, as it can capture spatial hierarchies in the image data. The comparison between PIL and non-PIL versions allows for analysis of any performance differences in data loading and preprocessing approaches."
      ],
      "metadata": {
        "id": "eoxX8tAFIm10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import time"
      ],
      "metadata": {
        "id": "0pK9sbAvKwM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **How the filters (also called kernels) work in this CNN and how their dimensions are determined:**\n",
        "\n",
        "**1. First Convolutional Layer (self.conv1):**\n",
        "   ```python\n",
        "   self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "   ```\n",
        "   This creates 32 filters, each of size 3x3, that operate on a single input channel.\n",
        "\n",
        "   - Input: 1 channel (grayscale MNIST images)\n",
        "   - Output: 32 channels\n",
        "   - Filter size: 3x3\n",
        "   - Stride: 1\n",
        "\n",
        "   How it works:\n",
        "   - Each 3x3 filter slides over the 28x28 input image.\n",
        "   - At each position, it performs element-wise multiplication and summation.\n",
        "   - This creates 32 feature maps, each slightly smaller than the input (26x26) due to the filter size.\n",
        "\n",
        "Let's break down this statement about the first convolutional layer (self.conv1) in detail:\n",
        "\n",
        "\"This creates 32 filters, each of size 3x3, that operate on a single input channel.\"\n",
        "\n",
        "1. Number of Filters: 32\n",
        "   - The layer creates 32 distinct filters.\n",
        "   - Each filter will produce one feature map in the output.\n",
        "   - This means the output of this layer will have 32 channels.\n",
        "\n",
        "2. Filter Size: 3x3\n",
        "   - Each filter is a small 2D matrix of size 3 rows by 3 columns.\n",
        "   - This size defines the receptive field of the filter - how much of the input it \"sees\" at once.\n",
        "\n",
        "3. Single Input Channel\n",
        "   - This refers to the input being a grayscale image (like MNIST digits).\n",
        "   - Grayscale images have only one channel, representing intensity.\n",
        "   - If it were an RGB image, we'd have 3 input channels.\n",
        "\n",
        "4. How it Works:\n",
        "   - Each 3x3 filter slides over the entire input image.\n",
        "   - At each position, it performs a dot product operation:\n",
        "     * Multiply each filter value with the corresponding image pixel.\n",
        "     * Sum up all these multiplications.\n",
        "     * Add a bias term.\n",
        "     * Apply an activation function (like ReLU).\n",
        "   - This process creates one value in the output feature map.\n",
        "\n",
        "5. Output Dimension:\n",
        "   - If the input is 28x28 (like MNIST), the output will be 26x26x32.\n",
        "   - The spatial dimensions shrink by 2 in each direction due to the 3x3 filter.\n",
        "   - We get 32 of these 26x26 feature maps, one from each filter.\n",
        "\n",
        "6. Purpose:\n",
        "   - Each filter learns to detect a specific pattern or feature (e.g., edges, textures).\n",
        "   - Having 32 filters allows the network to detect 32 different types of features.\n",
        "\n",
        "7. Parameters:\n",
        "   - Each filter has 3 * 3 = 9 weights, plus 1 bias term.\n",
        "   - Total parameters for this layer: (3 * 3 + 1) * 32 = 320\n",
        "\n",
        "8. Intuition:\n",
        "   - Think of each filter as a \"feature detector\" sliding over the image.\n",
        "   - It's looking for a specific pattern, and it lights up (activates) when it finds that pattern.\n",
        "\n",
        "This layer is the first step in transforming the raw pixel data into more abstract features, which subsequent layers will further process and combine to eventually classify the image.\n",
        "\n",
        "**2. Second Convolutional Layer (self.conv2):**\n",
        "   ```python\n",
        "   self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "   ```\n",
        "   This creates 64 filters, each of size 3x3, that operate on 32 input channels.\n",
        "\n",
        "   - Input: 32 channels (from previous layer)\n",
        "   - Output: 64 channels\n",
        "   - Filter size: 3x3\n",
        "   - Stride: 1\n",
        "\n",
        "   How it works:\n",
        "   - Each filter is actually 3x3x32 (matching the input depth).\n",
        "   - It slides over the 32 input feature maps, combining information across all channels.\n",
        "   - The output is 64 feature maps, further reduced in size (24x24).\n",
        "\n",
        "  When we say \"Each filter is actually 3x3x32\", we're referring to the structure of the filters in the second convolutional layer (self.conv2). This structure is crucial to understand how convolutional layers process multi-channel inputs. Let's break it down:\n",
        "\n",
        "1. Filter Dimensions:\n",
        "   - The filter is 3x3 in spatial dimensions (height and width).\n",
        "   - It has a depth of 32, matching the number of channels from the previous layer's output.\n",
        "\n",
        "2. Why 32 channels?\n",
        "   - The first convolutional layer (self.conv1) outputs 32 feature maps.\n",
        "   - These 32 feature maps become the input channels for the second layer.\n",
        "\n",
        "3. Structure of the Filter:\n",
        "   - Each filter in conv2 is not just a 2D matrix, but a 3D volume: 3 (height) x 3 (width) x 32 (depth).\n",
        "   - You can think of it as 32 separate 3x3 matrices stacked together.\n",
        "\n",
        "4. How it Works:\n",
        "   - When this 3x3x32 filter slides over the input, it processes all 32 input channels simultaneously.\n",
        "   - For each position, it performs 3x3x32 = 288 multiplications (plus a bias term).\n",
        "   - The results are summed up to produce a single value in the output feature map.\n",
        "\n",
        "5. Conceptual View:\n",
        "   - If you were to \"unroll\" this 3D filter, you'd have a single row of 3 * 3 * 32 = 288 weights.\n",
        "   - This allows the filter to capture patterns that span across all input channels.\n",
        "\n",
        "6. Number of Parameters:\n",
        "   - Each filter in conv2 has 3 * 3 * 32 + 1 = 289 parameters (including the bias).\n",
        "   - With 64 such filters, we get (3 * 3 * 32 + 1) * 64 = 18,496 parameters in total for this layer.\n",
        "\n",
        "7. Importance:\n",
        "   - This 3D structure allows the network to learn features that combine information from all input channels.\n",
        "   - It's how CNNs can detect complex patterns that aren't visible in any single channel alone.\n",
        "\n",
        "In essence, the 3x3x32 structure of each filter in the second layer enables the CNN to process and combine information from all 32 feature maps produced by the first layer. This allows for the detection of increasingly complex and abstract features as we go deeper into the network.\n",
        "\n",
        "**Transition from the output of conv1 to the input of conv2**\n",
        "\n",
        "1. Output of conv1:\n",
        "   - We have 32 feature maps, each of size 26x26.\n",
        "   - This can be thought of as a 3D volume: 26 x 26 x 32.\n",
        "\n",
        "2. Input to conv2:\n",
        "   - This entire 26 x 26 x 32 volume becomes the input to conv2.\n",
        "   - Each of the 32 feature maps is treated as an \"input channel\" for conv2.\n",
        "\n",
        "3. How conv2 processes this input:\n",
        "   - Each filter in conv2 is 3x3x32 in size.\n",
        "   - The '32' in 3x3x32 corresponds to the 32 input channels (feature maps) from conv1.\n",
        "   - Each filter in conv2 slides over all 32 input channels simultaneously.\n",
        "\n",
        "4. Computation in conv2:\n",
        "   - At each position, the 3x3x32 filter performs element-wise multiplication with a 3x3x32 patch of the input.\n",
        "   - These 3 * 3 * 32 = 288 multiplications are summed up (along with a bias term).\n",
        "   - This sum produces a single value in one of conv2's output feature maps.\n",
        "\n",
        "5. Dimensionality:\n",
        "   - Input to conv2: 26 x 26 x 32\n",
        "   - Each filter in conv2: 3 x 3 x 32\n",
        "   - Output of conv2: 24 x 24 x 64 (assuming 64 filters in conv2)\n",
        "\n",
        "6. Preserving spatial relationships:\n",
        "   - The spatial relationship between features in the 32 input channels is preserved.\n",
        "   - This allows conv2 to detect higher-level features that combine patterns from multiple conv1 feature maps.\n",
        "\n",
        "7. Analogy:\n",
        "   - If conv1's feature maps detected simple edges and textures, conv2 can combine these to detect more complex shapes or patterns.\n",
        "   - It's like going from detecting lines (conv1) to detecting combinations of lines that form specific shapes (conv2).\n",
        "\n",
        "8. Importance:\n",
        "   - This transition allows the network to build a hierarchy of features.\n",
        "   - Each subsequent layer can detect increasingly complex and abstract patterns.\n",
        "\n",
        "In essence, the 32 feature maps from conv1 are not \"converted\" in the traditional sense. Rather, they are used as a multi-channel input for conv2, allowing the network to build upon the features detected in the first layer to create more complex feature representations in the second layer.\n",
        "\n",
        "3. Dimension Calculations:\n",
        "   - Input image: 28x28\n",
        "   - After conv1: 26x26 (28 - 3 + 1 = 26)\n",
        "   - After conv2: 24x24 (26 - 3 + 1 = 24)\n",
        "   - After max pooling: 12x12 (24 / 2 = 12)\n",
        "\n",
        "   The formula for output size is:\n",
        "   Output size = (Input size - Filter size + 2 * Padding) / Stride + 1\n",
        "\n",
        "4. Number of Parameters:\n",
        "   - conv1: (3 * 3 * 1 + 1) * 32 = 320 parameters\n",
        "     (filter size * input channels + bias) * number of filters\n",
        "   - conv2: (3 * 3 * 32 + 1) * 64 = 18,496 parameters\n",
        "\n",
        "5. Flattening for Fully Connected Layer:\n",
        "   ```python\n",
        "   x = torch.flatten(x, 1)\n",
        "   ```\n",
        "   This flattens the output of conv2 after max pooling:\n",
        "   12 * 12 * 64 = 9,216 (which is the input size for fc1)\n",
        "\n",
        "The filters work by detecting specific patterns or features in the input. In early layers, they might detect simple features like edges or textures. In deeper layers, they can detect more complex, abstract features. The use of multiple filters allows the network to learn a diverse set of features at each layer.\n",
        "\n",
        "The dimensionality reduction through convolutions and pooling helps in:\n",
        "1. Reducing the number of parameters, preventing overfitting.\n",
        "2. Increasing the receptive field, allowing later layers to \"see\" more of the original input.\n",
        "3. Building a hierarchy of features, from simple to complex.\n",
        "\n",
        "This structure is what gives CNNs their power in image-related tasks, as it mirrors the hierarchical nature of visual information processing."
      ],
      "metadata": {
        "id": "rIJkL-zaK2W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "hk--OP4lKzqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PIL and Non PIL Versions"
      ],
      "metadata": {
        "id": "E40S-w2DTfdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PIL version\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, mnist_data, transform=None):\n",
        "        self.mnist_data = mnist_data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.mnist_data[index]\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_dataset_pil = MNISTDataset(train_data, transform=transform)\n",
        "test_dataset_pil = MNISTDataset(test_data, transform=transform)\n",
        "\n",
        "train_loader_pil = DataLoader(dataset=train_dataset_pil, batch_size=64, shuffle=True)\n",
        "test_loader_pil = DataLoader(dataset=test_dataset_pil, batch_size=64, shuffle=False)\n",
        "\n",
        "# Non-PIL version\n",
        "train_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)\n",
        "test_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader_no_pil = DataLoader(dataset=train_dataset_no_pil, batch_size=64, shuffle=True)\n",
        "test_loader_no_pil = DataLoader(dataset=test_dataset_no_pil, batch_size=64, shuffle=False)\n",
        "\n",
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# Testing function\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
        "    return accuracy\n",
        "\n",
        "# Training and testing for PIL version\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pil = Net().to(device)\n",
        "optimizer_pil = optim.Adam(model_pil.parameters())\n",
        "\n",
        "start_time_pil = time.time()\n",
        "for epoch in range(1, 6):\n",
        "    train(model_pil, device, train_loader_pil, optimizer_pil, epoch)\n",
        "accuracy_pil = test(model_pil, device, test_loader_pil)\n",
        "end_time_pil = time.time()\n",
        "training_time_pil = end_time_pil - start_time_pil\n",
        "\n",
        "# Training and testing for non-PIL version\n",
        "model_no_pil = Net().to(device)\n",
        "optimizer_no_pil = optim.Adam(model_no_pil.parameters())\n",
        "\n",
        "start_time_no_pil = time.time()\n",
        "for epoch in range(1, 6):\n",
        "    train(model_no_pil, device, train_loader_no_pil, optimizer_no_pil, epoch)\n",
        "accuracy_no_pil = test(model_no_pil, device, test_loader_no_pil)\n",
        "end_time_no_pil = time.time()\n",
        "training_time_no_pil = end_time_no_pil - start_time_no_pil\n",
        "\n",
        "# Results comparison\n",
        "print(\"\\n================= Comparison =================\")\n",
        "print(f\"Training Time (PIL): {training_time_pil:.2f} seconds\")\n",
        "print(f\"Training Time (No PIL): {training_time_no_pil:.2f} seconds\")\n",
        "print(f\"Test Accuracy (PIL): {accuracy_pil:.2f}%\")\n",
        "print(f\"Test Accuracy (No PIL): {accuracy_no_pil:.2f}%\")"
      ],
      "metadata": {
        "id": "-ahqLZz6Ik3I",
        "outputId": "7f117a2a-96ef-433c-8864-e045059ea397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 43379971.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1152093.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9632737.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10308727.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315129\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.236749\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.358660\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.121875\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.192088\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.225983\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.045553\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.212121\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.122635\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.130446\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.189344\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.302952\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.066816\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.078993\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.218973\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.079290\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.080326\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.071727\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.058419\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.060970\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.060682\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.090756\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.134420\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.020077\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.141956\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.010498\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.024725\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.050420\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.034910\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.443656\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.094093\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.020053\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.025128\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.023523\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.038402\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.077643\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.066597\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.005866\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.089120\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.168288\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.055742\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.061221\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.022563\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.035673\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.007222\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.006565\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.297104\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.112615\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.026294\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.065134\n",
            "\n",
            "Test set: Average loss: 0.0318, Accuracy: 9900/10000 (99.00%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312419\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.312278\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.387047\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.192589\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.207742\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.089568\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.128014\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.039546\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.094323\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.157312\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.140612\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.060067\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.084559\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.011275\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.079123\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.176868\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.115693\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.066130\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.024329\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.060080\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.117213\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.115177\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.106516\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.059329\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.023844\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.193843\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.021870\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.031574\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.062394\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.083735\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.038986\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.131309\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.128656\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.030955\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.052790\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.072336\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.070976\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.056397\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.031402\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.015194\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.055399\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.049902\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.025398\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.075316\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.037452\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.079206\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.038674\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.131314\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.032508\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.008290\n",
            "\n",
            "Test set: Average loss: 0.0279, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "\n",
            "================= Comparison =================\n",
            "Training Time (PIL): 127.01 seconds\n",
            "Training Time (No PIL): 74.76 seconds\n",
            "Test Accuracy (PIL): 99.00%\n",
            "Test Accuracy (No PIL): 99.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**CNN Workflow**\n",
        "This workflow allows the network to progressively transform raw pixel data into increasingly abstract and task-relevant features, culminating in a classification decision. Each stage plays a crucial role in the overall learning and inference process of the CNN.\n",
        "\n",
        "1. Input:\n",
        "   - Start with an input image, e.g., a 28x28 grayscale MNIST digit.\n",
        "\n",
        "2. Convolutional Layers:\n",
        "   - Conv1:\n",
        "     * Input: 28x28x1\n",
        "     * Apply 32 filters of size 3x3\n",
        "     * Output: 26x26x32 feature maps\n",
        "   - ReLU Activation:\n",
        "     * Applies element-wise ReLU to introduce non-linearity\n",
        "   - Conv2:\n",
        "     * Input: 26x26x32\n",
        "     * Apply 64 filters of size 3x3\n",
        "     * Output: 24x24x64 feature maps\n",
        "   - ReLU Activation:\n",
        "     * Again, apply element-wise ReLU\n",
        "\n",
        "3. Pooling Layer:\n",
        "   - Max Pooling:\n",
        "     * Input: 24x24x64\n",
        "     * Use 2x2 max pooling with stride 2\n",
        "     * Output: 12x12x64\n",
        "   - Purpose: Reduce spatial dimensions, retain important features\n",
        "\n",
        "4. Flattening:\n",
        "   - Input: 12x12x64\n",
        "   - Flatten operation: Reshape to 1D vector\n",
        "   - Output: 9216 (12 * 12 * 64) dimensional vector\n",
        "   - Purpose: Prepare convolutional output for fully connected layers\n",
        "\n",
        "5. Fully Connected (Linear) Layers:\n",
        "   - FC1:\n",
        "     * Input: 9216 dimensional vector\n",
        "     * Apply linear transformation to 128 neurons\n",
        "     * Output: 128 dimensional vector\n",
        "   - ReLU Activation:\n",
        "     * Apply element-wise ReLU\n",
        "   - Dropout:\n",
        "     * Randomly zero out some neurons to prevent overfitting\n",
        "   - FC2 (Output Layer):\n",
        "     * Input: 128 dimensional vector\n",
        "     * Apply linear transformation to 10 neurons (for 10 digit classes)\n",
        "     * Output: 10 dimensional vector\n",
        "\n",
        "6. Softmax:\n",
        "   - Input: 10 dimensional vector\n",
        "   - Apply softmax function\n",
        "   - Output: 10 probabilities (sum to 1) representing the likelihood of each digit class\n",
        "\n",
        "Workflow Summary:\n",
        "1. Convolutions extract spatial features from the input image.\n",
        "2. Pooling reduces the spatial dimensions while retaining important features.\n",
        "3. Flattening converts the 3D feature maps into a 1D vector.\n",
        "4. Fully connected layers combine these features for high-level reasoning.\n",
        "5. Softmax converts the final layer outputs into class probabilities.\n",
        "\n",
        "Key Points:\n",
        "- Convolutions and pooling work in the image's spatial domain.\n",
        "- Flattening bridges the gap between spatial (convolutions) and non-spatial (fully connected) processing.\n",
        "- Fully connected layers perform high-level feature combination and classification.\n",
        "- Softmax ensures the network outputs valid probabilities for multi-class classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "LBhncug1Y7UM"
      }
    }
  ]
}