{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOaeW9hWL+bQVzIm55tOYtx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/IntroToDeepLearning/blob/main/CNNwithAndWithoutPIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Explanation of the Code**\n",
        "\n",
        "This code is designed to compare two approaches for loading and processing the MNIST dataset in PyTorch: one using the `PIL` library for image handling and the other directly using PyTorch's built-in tensor handling through `torchvision.datasets.MNIST`. Both approaches involve training a simple neural network to classify handwritten digits from the MNIST dataset, and the results are compared in terms of training time and test accuracy.\n",
        "\n",
        "Let's go through the code step by step.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Importing Required Libraries**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "```\n",
        "\n",
        "- **`torch`**: Core PyTorch library.\n",
        "- **`torch.nn`**: Provides modules to build neural networks.\n",
        "- **`torch.optim`**: Contains optimizers like SGD, Adam, etc.\n",
        "- **`torch.nn.functional`**: Contains functions like `relu` and `cross_entropy` that are commonly used in neural networks.\n",
        "- **`torch.utils.data.DataLoader`**: Loads datasets in batches during training.\n",
        "- **`torchvision.datasets`**: Provides popular datasets like MNIST.\n",
        "- **`torchvision.transforms`**: Contains functions to transform data, such as converting images to tensors and normalizing them.\n",
        "- **`PIL`**: Used for handling image files manually.\n",
        "- **`time`**: For tracking execution time of training loops.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dst78QL6AKrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import time\n"
      ],
      "metadata": {
        "id": "Dg7DU2doALVI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **2. Version 1: Using PIL for Image Handling**\n",
        "\n",
        "#### Custom Dataset Class\n",
        "\n",
        "```python\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, mnist_data, transform=None):\n",
        "        self.mnist_data = mnist_data  # Store the MNIST dataset from torchvision.\n",
        "        self.transform = transform    # Transformation like converting to tensor and normalization.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)  # Return the number of images in the dataset.\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.mnist_data[index]  # Get an image and its label.\n",
        "        img = transforms.ToPILImage()(img)   # Convert the tensor image back to a PIL image.\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)  # Apply the specified transformations (to tensor, normalization).\n",
        "\n",
        "        return img, label  # Return the transformed image and its label.\n",
        "```\n",
        "\n",
        "- **Purpose**: This class wraps the `torchvision.datasets.MNIST` dataset and allows the manual conversion of images from tensors back to `PIL` images for custom processing. It also applies transformations like converting the image back to a tensor and normalizing it.\n",
        "  \n",
        "#### Dataset Loading and Transformations\n",
        "\n",
        "```python\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the PIL image to a tensor.\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the data with the mean and std of MNIST.\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_dataset_pil = MNISTDataset(train_data, transform=transform)\n",
        "test_dataset_pil = MNISTDataset(test_data, transform=transform)\n",
        "\n",
        "train_loader_pil = DataLoader(dataset=train_dataset_pil, batch_size=64, shuffle=True)\n",
        "test_loader_pil = DataLoader(dataset=test_dataset_pil, batch_size=64, shuffle=False)\n",
        "```\n",
        "\n",
        "- **Transformations**:\n",
        "  - `ToTensor()`: Converts the PIL image to a PyTorch tensor.\n",
        "  - `Normalize((0.1307,), (0.3081,))`: Standard normalization for the MNIST dataset (mean and std are specific to MNIST).\n",
        "  \n",
        "- **Datasets**:\n",
        "  - `datasets.MNIST`: Automatically downloads and loads the MNIST dataset if it's not already available.\n",
        "  \n",
        "- **DataLoader**: The `DataLoader` is used to load the data in batches, with `shuffle=True` for the training data to randomize the order of images for better generalization.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vIItfry4HKRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Version 1: Using PIL                        #\n",
        "###############################################\n",
        "\n",
        "# Custom Dataset Class for MNIST using PIL and torchvision\n",
        "# This class wraps the torchvision MNIST dataset but loads images using PIL to allow for manual control over image processing.\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, mnist_data, transform=None):\n",
        "        self.mnist_data = mnist_data  # Store the dataset passed in (torchvision MNIST dataset).\n",
        "        self.transform = transform    # Transformation (like converting to tensors and normalizing).\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_data)  # Return the number of items in the dataset.\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get the image and label at the specified index from the original dataset.\n",
        "        img, label = self.mnist_data[index]\n",
        "\n",
        "        # Convert the image from a tensor back to a PIL image for further processing.\n",
        "        img = transforms.ToPILImage()(img)\n",
        "\n",
        "        # Apply any transformations (like converting back to a tensor and normalizing).\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Return the processed image and its corresponding label.\n",
        "        return img, label\n",
        "\n",
        "# Set up transforms (convert to tensor and normalize)\n",
        "# We need to convert the images to tensors and normalize them (mean and std values are specific to MNIST).\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor.\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the data with MNIST-specific mean and std.\n",
        "])\n",
        "\n",
        "# Download the MNIST dataset using torchvision.\n",
        "# The dataset will be downloaded if not already present in './mnist_data'.\n",
        "train_data = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Wrap the torchvision MNIST dataset with our custom dataset class, which uses PIL for image handling.\n",
        "train_dataset_pil = MNISTDataset(train_data, transform=transform)\n",
        "test_dataset_pil = MNISTDataset(test_data, transform=transform)\n",
        "\n",
        "# DataLoader for batching. Batching helps in loading a set of images at once during training.\n",
        "# Shuffle=True ensures that the training data is shuffled each epoch for better generalization.\n",
        "train_loader_pil = DataLoader(dataset=train_dataset_pil, batch_size=64, shuffle=True)\n",
        "test_loader_pil = DataLoader(dataset=test_dataset_pil, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CXDyQi5IFLm_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **3. Version 2: Without PIL for Image Handling**\n",
        "\n",
        "```python\n",
        "train_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)\n",
        "test_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader_no_pil = DataLoader(dataset=train_dataset_no_pil, batch_size=64, shuffle=True)\n",
        "test_loader_no_pil = DataLoader(dataset=test_dataset_no_pil, batch_size=64, shuffle=False)\n",
        "```\n",
        "\n",
        "- **Difference**: In this version, the MNIST dataset is directly handled by `torchvision.datasets.MNIST`. The images are loaded as tensors right from the start, so there's no need for manual conversion using `PIL`.\n",
        "\n",
        "- **Advantages**: This is more efficient when working with standard datasets like MNIST because the data is already prepared in tensor format.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6KzunubjHBpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Version 2: Without PIL                      #\n",
        "###############################################\n",
        "\n",
        "# In this version, we use the dataset directly as provided by torchvision, without wrapping it in a custom dataset class.\n",
        "\n",
        "# Transformations (convert to tensor and normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Directly convert the images to PyTorch tensors.\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the data (mean and std specific to MNIST).\n",
        "])\n",
        "\n",
        "# Download and load the MNIST dataset directly.\n",
        "# The dataset will be downloaded and directly loaded without any manual PIL processing.\n",
        "train_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)\n",
        "test_dataset_no_pil = datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader for batching. Same as the PIL version.\n",
        "train_loader_no_pil = DataLoader(dataset=train_dataset_no_pil, batch_size=64, shuffle=True)\n",
        "test_loader_no_pil = DataLoader(dataset=test_dataset_no_pil, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "eIoDcnaXDvAi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Neural Network Architecture**\n",
        "\n",
        "```python\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # Fully connected layer 1: from input size (28*28) to 128 neurons.\n",
        "        self.fc2 = nn.Linear(128, 64)     # Fully connected layer 2: from 128 neurons to 64.\n",
        "        self.fc3 = nn.Linear(64, 10)      # Output layer: 10 neurons for 10 digit classes.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the 28x28 image into a vector of size 784.\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first layer.\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation to the second layer.\n",
        "        x = self.fc3(x)          # No activation here (cross-entropy will handle softmax).\n",
        "        return x\n",
        "```\n",
        "\n",
        "- **Network Overview**:\n",
        "  - Input size is `28x28` (since MNIST images are 28x28 pixels).\n",
        "  - Two hidden layers with ReLU activation.\n",
        "  - The final output layer has 10 neurons (one for each digit class).\n",
        "\n",
        "- **Purpose**: The network takes in a flattened image, processes it through two fully connected layers with ReLU activation, and then outputs a vector of 10 scores (one for each digit).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QFEJeT__Gzwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Shared Neural Network Code                  #\n",
        "###############################################\n",
        "\n",
        "# Define a simple fully connected neural network for classification.\n",
        "# The model has three layers: two hidden layers with ReLU activation and one output layer for classification.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input size is 28*28 (since MNIST images are 28x28 pixels).\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # First fully connected layer, 128 neurons.\n",
        "        self.fc2 = nn.Linear(128, 64)     # Second fully connected layer, 64 neurons.\n",
        "        self.fc3 = nn.Linear(64, 10)      # Output layer, 10 neurons (for 10 digit classes).\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor (28x28 pixels) into a vector of size 784.\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the first layer.\n",
        "        x = F.relu(self.fc2(x))  # Apply ReLU activation to the second layer.\n",
        "        x = self.fc3(x)          # Output layer (no activation, as we'll use CrossEntropyLoss).\n",
        "        return x\n",
        "\n",
        "# Create separate models for the two versions (PIL and No PIL).\n",
        "model_pil = Net()      # Model for the PIL version.\n",
        "model_no_pil = Net()   # Model for the non-PIL version."
      ],
      "metadata": {
        "id": "r76Q_b_xGvaB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **5. Loss Function and Optimizer**\n",
        "\n",
        "```python\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification tasks.\n",
        "optimizer_pil = optim.SGD(model_pil.parameters(), lr=0.01)  # Optimizer for the PIL version.\n",
        "optimizer_no_pil = optim.SGD(model_no_pil.parameters(), lr=0.01)  # Optimizer for the non-PIL version.\n",
        "```\n",
        "\n",
        "- **`CrossEntropyLoss`**: This loss function is used for classification tasks. It combines `LogSoftmax` and `Negative Log Likelihood` in one function.\n",
        "- **`SGD` Optimizer**: Stochastic Gradient Descent is used for optimization. The learning rate is set to 0.01 for both models.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "T3R_viJhGtaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Use CrossEntropyLoss for classification tasks and SGD for optimization.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_pil = optim.SGD(model_pil.parameters(), lr=0.01)  # Optimizer for the PIL version.\n",
        "optimizer_no_pil = optim.SGD(model_no_pil.parameters(), lr=0.01)  # Optimizer for the non-PIL version.\n"
      ],
      "metadata": {
        "id": "eSXqt4uHD-n2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Training and Testing Loop for PIL Version**\n",
        "\n",
        "#### Training Loop\n",
        "\n",
        "```python\n",
        "start_time_pil = time.time()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model_pil.train()  # Set the model to training mode.\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader_pil):\n",
        "        optimizer_pil.zero_grad()  # Clear previous gradients.\n",
        "        output = model_pil(data)  # Forward pass through the network.\n",
        "        loss = criterion(output, target)  # Compute the loss.\n",
        "        loss.backward()  # Backward pass to compute gradients.\n",
        "        optimizer_pil.step()  # Update model weights.\n",
        "        running_loss += loss.item()  # Accumulate the loss.\n",
        "    \n",
        "    print(f'PIL Version - Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader_pil):.4f}')\n",
        "```\n",
        "\n",
        "- **Training Process**:\n",
        "  - **Forward pass**: The data is passed through the network to make predictions.\n",
        "  - **Loss calculation**: The difference between the predicted output and true labels is computed using cross-entropy loss.\n",
        "  - **Backward pass**: The gradients of the loss with respect to the model parameters are computed.\n",
        "  - **Optimization**: The optimizer updates the model parameters based on the gradients.\n",
        "\n",
        "#### Testing Loop\n",
        "\n",
        "```python\n",
        "model_pil.eval()  # Set the model to evaluation mode (no gradient calculation).\n",
        "correct_pil = 0\n",
        "total_pil = 0\n",
        "\n",
        "with torch.no_grad():  # No need to compute gradients during evaluation.\n",
        "    for data, target in test_loader_pil:\n",
        "        outputs = model_pil(data)  # Forward pass.\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the predicted class.\n",
        "        total_pil += target.size(0)  # Increment the total number of samples.\n",
        "        correct_pil += (predicted == target).sum().item()  # Count correct predictions.\n",
        "\n",
        "accuracy_pil = 100 * correct_pil / total_pil  # Compute accuracy.\n",
        "print(f'PIL Version - Test Accuracy: {accuracy_pil:.2f}%')\n",
        "```\n",
        "\n",
        "- **Evaluation**: The model is evaluated on the test set by making\n",
        "\n",
        " predictions, comparing them to the true labels, and calculating accuracy.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "oootkHEeGc9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################\n",
        "# Training and Testing for PIL Version        #\n",
        "###############################################\n",
        "\n",
        "# Record start time for the PIL version to measure training time.\n",
        "start_time_pil = time.time()\n",
        "\n",
        "# Training loop for PIL version\n",
        "for epoch in range(5):  # We train the model for 5 epochs.\n",
        "    model_pil.train()  # Set the model to training mode.\n",
        "    running_loss = 0.0  # Variable to track loss over the epoch.\n",
        "\n",
        "    # Loop over batches of data in the training set.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader_pil):\n",
        "        optimizer_pil.zero_grad()  # Zero the gradients (required before every backward pass).\n",
        "        output = model_pil(data)   # Forward pass: get predictions from the model.\n",
        "        loss = criterion(output, target)  # Calculate the loss (how far predictions are from true labels).\n",
        "        loss.backward()  # Backward pass: compute gradients.\n",
        "        optimizer_pil.step()  # Update model weights based on gradients.\n",
        "        running_loss += loss.item()  # Accumulate the loss.\n",
        "\n",
        "    # Print the average loss for the epoch.\n",
        "    print(f'PIL Version - Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader_pil):.4f}')\n",
        "\n",
        "# Testing the model for PIL version\n",
        "model_pil.eval()  # Set the model to evaluation mode (no backpropagation, etc.).\n",
        "correct_pil = 0   # To count how many predictions were correct.\n",
        "total_pil = 0     # To count the total number of examples.\n",
        "\n",
        "# Loop through the test dataset.\n",
        "with torch.no_grad():  # No need to compute gradients during evaluation.\n",
        "    for data, target in test_loader_pil:\n",
        "        outputs = model_pil(data)  # Forward pass: get predictions.\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the highest score as the prediction.\n",
        "        total_pil += target.size(0)  # Increment the total number of examples.\n",
        "        correct_pil += (predicted == target).sum().item()  # Count correct predictions.\n",
        "\n",
        "# Record end time for the PIL version.\n",
        "end_time_pil = time.time()\n",
        "training_time_pil = end_time_pil - start_time_pil  # Calculate total training time.\n",
        "accuracy_pil = 100 * correct_pil / total_pil  # Calculate accuracy as a percentage.\n",
        "\n",
        "print(f'PIL Version - Test Accuracy: {accuracy_pil:.2f}%')\n",
        "print(f'PIL Version - Training Time: {training_time_pil:.2f} seconds')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd72OT5sED68",
        "outputId": "ab757867-257f-4c37-93a0-c97f5b810fc3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIL Version - Epoch 1, Training Loss: 0.8008\n",
            "PIL Version - Epoch 2, Training Loss: 0.3135\n",
            "PIL Version - Epoch 3, Training Loss: 0.2565\n",
            "PIL Version - Epoch 4, Training Loss: 0.2187\n",
            "PIL Version - Epoch 5, Training Loss: 0.1904\n",
            "PIL Version - Test Accuracy: 94.78%\n",
            "PIL Version - Training Time: 117.36 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################\n",
        "# Training and Testing for No PIL Version     #\n",
        "###############################################\n",
        "\n",
        "# Record start time for the non-PIL version.\n",
        "start_time_no_pil = time.time()\n",
        "\n",
        "# Training loop for no PIL version (same as PIL version, but using the non-PIL data loader).\n",
        "for epoch in range(5):\n",
        "    model_no_pil.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Loop over batches of data in the training set.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader_no_pil):\n",
        "        optimizer_no_pil.zero_grad()  # Zero the gradients.\n",
        "        output = model_no_pil(data)   # Forward pass.\n",
        "        loss = criterion(output, target)  # Calculate the loss.\n",
        "        loss.backward()  # Backward pass: compute gradients.\n",
        "        optimizer_no_pil.step()  # Update model weights.\n",
        "        running_loss += loss.item()  # Accumulate the loss.\n",
        "\n",
        "    # Print the average loss for the epoch.\n",
        "    print(f'No PIL Version - Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader_no_pil):.4f}')\n",
        "\n",
        "# Testing the model for no PIL version\n",
        "model_no_pil.eval()  # Set the model to evaluation mode.\n",
        "correct_no_pil = 0   # To count how many predictions were correct.\n",
        "total_no_pil = 0     # To count the total number of examples.\n",
        "\n",
        "# Loop through the test dataset.\n",
        "with torch.no_grad():  # No need to compute gradients during evaluation.\n",
        "    for data, target in test_loader_no_pil:\n",
        "        outputs = model_no_pil(data)  # Forward pass.\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the highest score as the prediction.\n",
        "        total_no_pil += target.size(0)  # Increment the total number of examples.\n",
        "        correct_no_pil += (predicted == target).sum().item()  # Count correct predictions.\n",
        "\n",
        "# Record end time for the non-PIL version.\n",
        "end_time_no_pil = time.time()\n",
        "training_time_no_pil = end_time_no_pil - start_time_no_pil  # Calculate total training time.\n",
        "accuracy_no_pil = 100 * correct_no_pil / total_no_pil  # Calculate accuracy as a percentage.\n",
        "\n",
        "print(f'No PIL Version - Test Accuracy: {accuracy_no_pil:.2f}%')\n",
        "print(f'No PIL Version - Training Time: {training_time_no_pil:.2f} seconds')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR8O3x2aEJBU",
        "outputId": "0c0313de-72ab-4671-e361-356b95b94d02"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No PIL Version - Epoch 1, Training Loss: 0.8548\n",
            "No PIL Version - Epoch 2, Training Loss: 0.3192\n",
            "No PIL Version - Epoch 3, Training Loss: 0.2609\n",
            "No PIL Version - Epoch 4, Training Loss: 0.2233\n",
            "No PIL Version - Epoch 5, Training Loss: 0.1952\n",
            "No PIL Version - Test Accuracy: 94.50%\n",
            "No PIL Version - Training Time: 68.90 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Timing and Comparison**\n",
        "\n",
        "```python\n",
        "# Timing and accuracy are tracked for both versions.\n",
        "end_time_pil = time.time()\n",
        "training_time_pil = end_time_pil - start_time_pil\n",
        "\n",
        "print(f'PIL Version - Training Time: {training_time_pil:.2f} seconds')\n",
        "\n",
        "# Repeat the same for the non-PIL version.\n",
        "```\n",
        "\n",
        "- **Timing**: The `time.time()` function is used to measure how long it takes to train the model for both versions. This allows a direct comparison of training times.\n",
        "\n",
        "- **Results Comparison**:\n",
        "  - Training times and accuracies for both the `PIL` and non-`PIL` versions are printed side by side to compare the performance of each approach.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fOwGUmksGNe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# Results Comparison                          #\n",
        "###############################################\n",
        "\n",
        "# Print a side-by-side comparison of the training times and test accuracies.\n",
        "print(\"\\n================= Comparison =================\")\n",
        "print(f\"Training Time (PIL): {training_time_pil:.2f} seconds\")\n",
        "print(f\"Training Time (No PIL): {training_time_no_pil:.2f} seconds\")\n",
        "print(f\"Test Accuracy (PIL): {accuracy_pil:.2f}%\")\n",
        "print(f\"Test Accuracy (No PIL): {accuracy_no_pil:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0XeINOWELBt",
        "outputId": "7fbf0c2b-b6bb-4aa0-8b6f-ad6d68884695"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================= Comparison =================\n",
            "Training Time (PIL): 117.36 seconds\n",
            "Training Time (No PIL): 68.90 seconds\n",
            "Test Accuracy (PIL): 94.78%\n",
            "Test Accuracy (No PIL): 94.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using PIL and not using PIL**\n",
        "\n",
        "| **Aspect**                  | **Using PIL**                                      | **Without PIL**                                     |\n",
        "|-----------------------------|----------------------------------------------------|----------------------------------------------------|\n",
        "| **Image Handling**           | Converts image tensors back to **PIL** images. This allows for custom image processing using `PIL` (Python Imaging Library). | Directly uses tensors from the dataset without converting to PIL. This avoids the overhead of image format conversions. |\n",
        "| **Custom Dataset Class**     | Requires a custom `MNISTDataset` class to load images using `PIL` and apply transformations manually. | Does not require a custom dataset class. The `torchvision.datasets.MNIST` dataset is directly used as tensors. |\n",
        "| **Transformations**          | The images are first converted back to **PIL** images and then transformed back to tensors using `transforms.ToTensor()`. This allows for more flexibility with custom image handling if needed. | Transformations (e.g., `ToTensor()` and normalization) are applied directly to the image tensors using `torchvision.transforms`. No need for PIL-based image transformations. |\n",
        "| **Efficiency**               | **Less efficient**: Converting images from tensors to **PIL** and then back to tensors introduces overhead, making this approach slower, especially for large datasets. | **More efficient**: Directly working with tensors avoids unnecessary conversions, making it faster and more suitable for large-scale datasets like MNIST. |\n",
        "| **Flexibility**              | **More flexible**: If custom image processing (e.g., resizing, cropping, augmentations) is required, using `PIL` allows for advanced image manipulation that isn't always available in `torchvision.transforms`. | **Less flexible**: `torchvision.transforms` is powerful for common image processing needs, but it might not cover advanced or specific custom operations that **PIL** can handle. |\n",
        "| **Code Complexity**          | **Higher complexity**: Requires a custom dataset class to manage PIL conversions and manual handling of transformations. This adds extra code and complexity. | **Lower complexity**: Simply using `torchvision.datasets.MNIST` directly with transformations reduces code complexity, making it easier to implement and maintain. |\n",
        "| **Use Case Suitability**     | Suitable if you need **custom image preprocessing** or manipulation (e.g., resizing, filtering, augmentation) before converting to tensors. Common in projects requiring advanced preprocessing beyond normalization or conversion. | Suitable for most standard datasets where the focus is on efficient loading and training. Common in projects where you need fast, **out-of-the-box dataset handling**, especially for widely used datasets like MNIST. |\n",
        "| **Training Time**            | Takes longer due to the additional conversion steps between tensor and PIL images. This extra step increases the overall training time, especially noticeable with large datasets or high epochs. | Faster since the images are handled as tensors directly. Avoiding the PIL conversion reduces unnecessary overhead, improving training time. |\n",
        "| **Code Maintenance**         | More difficult to maintain, especially if adding or modifying the transformations requires working through a custom dataset class. | Easier to maintain since you rely on PyTorch's well-documented and widely-used data handling functionality. |\n",
        "| **Memory Overhead**          | Higher memory usage since each image is converted between formats, which can be taxing when working with large datasets. | Lower memory overhead since the data remains in tensor format, which is native to PyTorch and more memory efficient. |\n",
        "| **Transform Customizability** | Provides full control over how images are loaded, processed, and transformed. You can create custom pipelines involving PIL methods before converting to a tensor. | Less customizable but still allows common transformations like normalization, resizing, and data augmentation with `torchvision.transforms`. Custom transformations can still be added but in a more restricted environment. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points:**\n",
        "\n",
        "1. **Performance**:\n",
        "   - The **without PIL** approach is faster and more efficient because it skips the unnecessary step of converting between image formats. By directly handling the images as tensors, this method allows for faster data loading, training, and testing, particularly important in large-scale datasets.\n",
        "\n",
        "2. **Flexibility**:\n",
        "   - The **using PIL** approach offers more flexibility for custom image manipulation. For instance, if you need to perform advanced image preprocessing, like applying filters, specific augmentations, or detailed custom transformations, the PIL approach gives you more control.\n",
        "   - However, **without PIL** is still capable of common transformations like resizing, normalization, and flipping, but it's more constrained to the functionalities provided by `torchvision.transforms`.\n",
        "\n",
        "3. **Complexity**:\n",
        "   - **Using PIL** adds complexity because it requires creating a custom dataset class and manually handling image conversions. This additional code increases the risk of bugs and makes the code more difficult to maintain.\n",
        "   - **Without PIL** is simpler and easier to manage since you're using PyTorchâ€™s built-in functions for handling datasets and transformations.\n",
        "\n",
        "4. **Use Cases**:\n",
        "   - **Using PIL** is more appropriate when working with custom datasets where you might need non-standard image preprocessing steps.\n",
        "   - **Without PIL** is ideal for standard tasks like MNIST classification, where the dataset is already structured and doesn't require complex image manipulations. This approach is faster and easier to implement.\n",
        "\n",
        "---\n",
        "\n",
        "### **Which Approach Should You Use?**\n",
        "\n",
        "- **Use `PIL`** when:\n",
        "  - You need **advanced image preprocessing**.\n",
        "  - You're working with **custom datasets** that require custom image handling.\n",
        "  - You want **fine-grained control** over how images are loaded and processed.\n",
        "\n",
        "- **Skip `PIL` (Use tensors directly)** when:\n",
        "  - You're working with **standard datasets** like MNIST, CIFAR, etc.\n",
        "  - You prioritize **efficiency** and **simplicity**.\n",
        "  - You want to reduce **code complexity** and **training time**.\n",
        "\n",
        "In conclusion, for most typical scenarios like MNIST classification, **not using PIL** is the better choice due to its efficiency, simplicity, and ease of use. However, **using PIL** offers more control when you need custom processing for complex datasets."
      ],
      "metadata": {
        "id": "N3EQbH6_IpeB"
      }
    }
  ]
}