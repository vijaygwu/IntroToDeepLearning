{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7GK1aB2TfptPqMJUzdD+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/IntroToDeepLearning/blob/main/NumPyAndTensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NumPy: The Foundation of Numerical Computing in Python\n",
        "****\n",
        "**Summary**\n",
        "\n",
        "NumPy provides a powerful, efficient, and user-friendly toolkit for working with numerical data in Python. Its widespread adoption and integration with other scientific libraries make it an essential tool for anyone involved in data analysis, scientific computing, or machine learning.\n",
        "\n",
        "**Introduction to Numpy**\n",
        "\n",
        "NumPy (Numerical Python) is a powerful open-source library that forms the backbone of scientific and numerical computing within the Python ecosystem. It provides:\n",
        "\n",
        "**1. The ndarray (N-dimensional array):**\n",
        "\n",
        "* **Efficient Data Structure:** NumPy's core strength lies in its `ndarray` (or `array`) object, a multidimensional array designed for fast and efficient numerical operations. It stores data in a contiguous block of memory, enabling highly optimized computations.\n",
        "* **Homogeneous Data:** Typically, all elements within an ndarray are of the same data type (e.g., integers, floats), which further enhances computational performance.\n",
        "\n",
        "**2. A Vast Collection of Functions:**\n",
        "\n",
        "* **Mathematical Operations:**  NumPy offers a rich set of mathematical functions for performing element-wise operations, linear algebra, Fourier transforms, random number generation, and much more, directly on arrays.\n",
        "* **Broadcasting:**  NumPy's broadcasting rules allow you to perform operations on arrays of different shapes seamlessly, eliminating the need for manual loops and making your code concise and efficient.\n",
        "* **Indexing and Slicing:** Powerful indexing and slicing mechanisms make it easy to access and manipulate specific elements or sub-arrays within the `ndarray`.\n",
        "\n",
        "**3. Interoperability:**\n",
        "\n",
        "* **Foundation for Other Libraries:**  NumPy acts as the foundational building block for many other scientific and data analysis libraries in Python. Libraries like SciPy, Pandas, Matplotlib, and many machine learning frameworks (like TensorFlow and PyTorch) heavily rely on NumPy arrays for efficient data handling and computation.\n",
        "\n",
        "**Key Advantages:**\n",
        "\n",
        "* **Performance:** NumPy's underlying implementation in C and Fortran delivers significant performance gains compared to working with standard Python lists, especially for large datasets.\n",
        "* **Ease of Use:** NumPy's intuitive and expressive syntax makes it relatively easy to perform complex numerical computations.\n",
        "* **Versatility:**  NumPy is used in a wide range of scientific and engineering domains, including data analysis, machine learning, image processing, signal processing, and more.\n"
      ],
      "metadata": {
        "id": "4dBpD16fL595"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a 2D array\n",
        "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Perform element-wise multiplication\n",
        "result = arr * 2\n",
        "print(result)  # Output: [[ 2  4  6]\n",
        "              #          [ 8 10 12]]\n",
        "\n",
        "# Calculate the mean of all elements\n",
        "mean_value = np.mean(arr)\n",
        "print(mean_value)  # Output: 3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F15YWDcMfgD",
        "outputId": "10dde995-ba5a-4acf-dc4f-7ac31144e765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  4  6]\n",
            " [ 8 10 12]]\n",
            "3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Types in NumPy\n",
        "\n",
        "****\n",
        "\n",
        "NumPy handles types in a few key ways that make it a powerful tool for numerical computation:\n",
        "\n",
        "**1. Homogeneous Arrays:**\n",
        "\n",
        "* **Fundamental Principle:** NumPy arrays are designed to be *homogeneous*, meaning all elements within an array have the same data type (e.g., `int32`, `float64`, `bool`).\n",
        "* **Efficiency:** This homogeneity allows NumPy to optimize storage and computation significantly.\n",
        "* **Data Type Specification:** When creating an array, you can specify the data type using the `dtype` argument. If not provided, NumPy attempts to infer the appropriate type based on the input data.\n",
        "\n",
        "**2. Type Casting and Promotion:**\n",
        "\n",
        "* **Automatic Type Conversion:** NumPy will often perform *type casting* or *type promotion* when you perform operations on arrays with different data types.\n",
        "    * **Casting:**  If possible, values are converted to a compatible type without loss of precision.\n",
        "    * **Promotion:** If casting isn't possible without losing information, values are promoted to a larger data type that can accommodate all values.\n",
        "\n",
        "**3. Data Type Objects (`dtype`)**\n",
        "\n",
        "* **Describing Scalar Types:** NumPy has a rich set of built-in scalar data types to describe various precisions of integers, floating-point numbers, booleans, and more.\n",
        "* **Structured Data Types:** You can create structured data types where fields contain other data types, providing a way to organize complex data within an array.\n",
        "* **Checking and Comparing:** Use the `dtype` attribute to access and compare the data type of an array. Remember to use `==` for comparison, not `is`.\n",
        "\n",
        "**4. Flexibility and Control:**\n",
        "\n",
        "* **Explicit Type Conversion:** You can use functions like `astype()` to explicitly convert an array to a different data type.\n",
        "* **Type Safety:** Be aware of potential issues when mixing types, like unintended integer division or overflow.\n",
        "* **Advanced Typing:**  NumPy 1.20+ introduces type annotations for improved static type checking with tools like `mypy`.\n",
        "\n",
        "\n",
        "\n",
        "**Key Takeaway:** NumPy's focus on homogeneous arrays and its smart type handling mechanisms allow for efficient computation while still offering the flexibility to work with various data types.\n"
      ],
      "metadata": {
        "id": "-dRYGKDBMmwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "arr1 = np.array([1, 2, 3])  # inferred as int64\n",
        "arr2 = np.array([1.5, 2.5, 3.5])  # inferred as float64\n",
        "\n",
        "# Type Promotion: Result is float64 to accommodate all values\n",
        "result = arr1 + arr2\n",
        "print(result)  # Output: [2.5 4.5 6.5]\n",
        "print(result.dtype) # Output: float64\n",
        "\n",
        "# Explicit Type Conversion\n",
        "int_result = result.astype(np.int32)  # Convert to int32\n",
        "print(int_result)  # Output: [2 4 6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Tsz4QWNd_t",
        "outputId": "245a8fc9-424d-4103-e8a3-d0d986b177b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.5 4.5 6.5]\n",
            "float64\n",
            "[2 4 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Tensor\n",
        "\n",
        "PyTorch tensors are the fundamental building blocks for representing and manipulating data in PyTorch. They provide the core functionalities for defining models, performing computations, and training neural networks within the PyTorch framework.\n",
        "\n",
        "\n",
        "**What are PyTorch Tensors?**\n",
        "\n",
        "At their core, PyTorch tensors are multidimensional arrays, similar to NumPy arrays, but with a few key distinctions:\n",
        "\n",
        "* **GPU Acceleration:**  PyTorch tensors can seamlessly leverage the computational power of GPUs (Graphical Processing Units), dramatically speeding up numerical computations, especially for deep learning tasks.\n",
        "* **Automatic Differentiation:** PyTorch tensors are built for automatic differentiation (autograd), a crucial feature for training neural networks where gradients are calculated automatically, facilitating efficient optimization.\n",
        "\n",
        "**Core Characteristics:**\n",
        "\n",
        "* **Multidimensional:** Tensors can represent scalars (0-dimensional), vectors (1-dimensional), matrices (2-dimensional), and higher-dimensional arrays.\n",
        "* **Homogeneous Data:** Typically, all elements within a tensor are of the same data type (e.g., float32, int64).\n",
        "* **Dynamic Computation Graph:** PyTorch constructs a dynamic computational graph as your code executes, keeping track of operations on tensors for backpropagation.\n",
        "* **Seamless NumPy Integration:** Tensors can be effortlessly converted to and from NumPy arrays, enabling easy interaction with the wider scientific computing ecosystem in Python.\n",
        "\n",
        "**Key Operations:**\n",
        "\n",
        "* **Creation:**  You can create tensors using various methods, including:\n",
        "    * `torch.tensor()` (from existing data)\n",
        "    * `torch.rand()`, `torch.randn()`, `torch.zeros()`, `torch.ones()` (with specific values)\n",
        "    * `torch.from_numpy()` (convert from NumPy arrays)\n",
        "* **Indexing and Slicing:**  Similar to NumPy, tensors support powerful indexing and slicing to access specific elements or sub-tensors.\n",
        "* **Mathematical Operations:** PyTorch offers a vast collection of mathematical functions operating directly on tensors (element-wise, linear algebra, etc.).\n",
        "* **Broadcasting:**  Tensors support broadcasting, allowing operations on tensors of different but compatible shapes.\n",
        "* **Shape Manipulation:**  You can reshape, transpose, and concatenate tensors to fit your needs.\n",
        "* **GPU Operations:** If a GPU is available, you can move tensors to the GPU using `.to('cuda')` for accelerated computation.\n",
        "\n",
        "**Advantages of PyTorch Tensors:**\n",
        "\n",
        "* **Intuitive:** Pythonic syntax for easy learning and usage.\n",
        "* **Flexible:** Dynamic graph construction for greater control and experimentation.\n",
        "* **Powerful:**  GPU acceleration and automatic differentiation for efficient deep learning.\n",
        "* **Widely Adopted:** PyTorch is a popular framework in the machine learning community.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rGMfTHbmNzVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "# Create a tensor\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "\n",
        "# Perform element-wise addition\n",
        "y = x + 1\n",
        "print(y)\n",
        "\n",
        "# Matrix multiplication\n",
        "z = torch.matmul(x, x.t())  # t() for transpose\n",
        "print(z)\n",
        "\n",
        "# Move to GPU (if available)\n",
        "if torch.cuda.is_available():\n",
        "    x = x.to('cuda')\n",
        "    print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfOV8BOzOAgg",
        "outputId": "1db6f7e8-7245-4192-fa77-532601fd403a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 3],\n",
            "        [4, 5]])\n",
            "tensor([[ 5, 11],\n",
            "        [11, 25]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Differences between NumPy and PyTorch Tensors\n",
        "****\n",
        "\n",
        "1. **GPU Acceleration:**\n",
        "\n",
        "   * **PyTorch Tensors:** A major advantage of PyTorch tensors is their seamless integration with GPUs. This enables you to perform massively parallel computations, drastically speeding up deep learning and other computationally intensive tasks.\n",
        "   * **NumPy Arrays:** NumPy arrays primarily operate on the CPU. While there are ways to leverage GPUs with NumPy (e.g., using libraries like CuPy), it's not as natively integrated as in PyTorch.\n",
        "\n",
        "2. **Automatic Differentiation:**\n",
        "\n",
        "   * **PyTorch Tensors:** Tensors are designed with built-in support for automatic differentiation (autograd), a crucial feature for training neural networks. PyTorch automatically tracks the operations performed on tensors, allowing it to compute gradients efficiently during backpropagation.\n",
        "   * **NumPy Arrays:** NumPy doesn't have built-in automatic differentiation. While there are libraries like Autograd that can add this capability, it's not a core feature of NumPy.\n",
        "\n",
        "3. **Dynamic vs. Static Computation Graphs:**\n",
        "\n",
        "   * **PyTorch Tensors:** PyTorch employs a dynamic computational graph. The graph is built on the fly as your code executes, offering greater flexibility for control flow, debugging, and experimenting with models.\n",
        "   * **NumPy Arrays:** NumPy focuses on array operations without explicitly building or managing a computational graph.\n",
        "\n",
        "4. **Deep Learning Integration:**\n",
        "\n",
        "   * **PyTorch Tensors:**  PyTorch tensors are the cornerstone of the PyTorch deep learning framework. They seamlessly integrate with neural network layers, loss functions, optimizers, and other core components.\n",
        "   * **NumPy Arrays:** While NumPy provides the essential numerical foundation for many deep learning libraries, it doesn't directly offer the higher-level abstractions for building and training neural networks.\n",
        "\n",
        "**When to Choose Which**\n",
        "\n",
        "* **PyTorch Tensors:** Ideal for deep learning tasks due to their GPU acceleration, automatic differentiation, and dynamic graph capabilities. They offer a natural fit within the PyTorch ecosystem.\n",
        "* **NumPy Arrays:** Excellent for general scientific computing, numerical analysis, and data manipulation tasks. They are widely supported across the scientific Python stack.\n",
        "\n",
        "**Interoperability**\n",
        "\n",
        "* **Conversion:** You can easily convert between NumPy arrays and PyTorch tensors using `torch.from_numpy()` and `.numpy()`. This allows you to leverage the strengths of both libraries when needed.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "Both NumPy arrays and PyTorch tensors are vital tools for scientific computing in Python.  \n",
        "\n",
        "* PyTorch tensors excel in deep learning tasks due to their GPU capabilities and automatic differentiation.\n",
        "* NumPy arrays are more general-purpose and perfect for numerical operations and data analysis.\n",
        "\n",
        "Choose the one that best suits your specific task and leverage their interoperability when required.\n"
      ],
      "metadata": {
        "id": "1HpoHQzCOGDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How are types handled in NumPy and Pytorch Tensor**\n",
        "****\n",
        "\n",
        "**NumPy Arrays**\n",
        "\n",
        "* **Homogeneity:** NumPy arrays are fundamentally designed to be homogeneous, where all elements within an array share the same data type (e.g., `int32`, `float64`). This facilitates optimized memory layout and computation.\n",
        "* **Type Inference:** When creating an array, NumPy tries to intelligently infer the appropriate data type based on the provided data. If you give it a mix of integers and floats, it'll likely choose `float64` to preserve precision.\n",
        "* **Explicit `dtype`:** You have the control to specify the data type during array creation using the `dtype` argument.\n",
        "* **Type Casting/Promotion:** NumPy handles operations between arrays with differing types through *casting* (converting to a compatible type without loss) or *promotion* (upgrading to a larger type that can accommodate all values).\n",
        "\n",
        "**PyTorch Tensors**\n",
        "\n",
        "* **Similar Homogeneity:** Like NumPy, PyTorch tensors also strive for homogeneity, aiming for all elements to have the same data type.\n",
        "* **Default `float32`:** By default, tensors are created with the `float32` data type, suitable for many machine learning tasks.\n",
        "* **`dtype` Control:**  You can explicitly specify the data type using the `dtype` argument when creating a tensor.\n",
        "* **Type Promotion:**  PyTorch employs type promotion to handle operations between tensors of different types. It aims to find the smallest data type that can represent all values without loss of precision.\n",
        "* **Automatic Type Conversion:** PyTorch often performs automatic type conversion when necessary for computations. For example, if you add an integer tensor to a float tensor, the integer tensor will be automatically converted to float.\n",
        "\n",
        "**Key Distinctions:**\n",
        "\n",
        "* **Strictness:** NumPy is generally stricter about type consistency. It'll raise errors more readily if you try to mix incompatible types within an array.\n",
        "* **Flexibility:** PyTorch tends to be more flexible, often performing automatic conversions behind the scenes to enable computations. This can be convenient but also requires you to be mindful of potential precision loss or unintended behavior.\n",
        "* **GPU Compatibility:** A defining feature of PyTorch tensors is their ability to reside and perform computations on GPUs. NumPy arrays, while powerful for CPU-based tasks, lack this native GPU support.\n",
        "\n",
        "\n",
        "**Remember:**\n",
        "\n",
        "* Both NumPy and PyTorch allow you to check and change data types using `dtype` and functions like `astype()` (NumPy) or `to()` (PyTorch).\n",
        "* Always be aware of potential type-related issues (like unintended integer division or overflow) when performing operations on arrays/tensors with mixed data types.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "raR20gMJO2x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "arr1 = np.array([1, 2, 3])  # int64\n",
        "arr2 = np.array([1.5, 2.5])  # float64\n",
        "\n",
        "# Type Promotion: Result is float64 to accommodate all values\n",
        "result = arr1[:2] + arr2  # [2.5 4.5]\n",
        "\n",
        "# Torch\n",
        "\n",
        "import torch\n",
        "\n",
        "tensor1 = torch.tensor([1, 2, 3])  # int64\n",
        "tensor2 = torch.tensor([1.5, 2.5])  # float32\n",
        "\n",
        "# Automatic Conversion: Integer tensor is converted to float32\n",
        "result = tensor1[:2] + tensor2  # tensor([2.5000, 4.5000])"
      ],
      "metadata": {
        "id": "p1qJai4zP-Sd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}