{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWMeL5A5j7RJ04s+TnAafB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/IntroToDeepLearning/blob/main/LayerNormVs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layernorm\n",
        "\n",
        "Layer Normalization (LayerNorm) is a normalization technique typically used in deep learning models, particularly in transformer models. Unlike Batch Normalization (which normalizes across the batch dimension), Layer Normalization normalizes across the feature dimension within each individual data point, making it more suitable for recurrent neural networks or transformer architectures where the batch size may be very small or vary in size.\n",
        "\n",
        "### How Layer Normalization Works:\n",
        "\n",
        "1. **Input to a Layer:**\n",
        "   Consider an input tensor \\( x \\) of shape \\( [N, D] \\), where:\n",
        "   - \\( N \\) is the number of data points (batch size).\n",
        "   - \\( D \\) is the number of features in each data point.\n",
        "\n",
        "2. **Normalization Step:**\n",
        "   For each data point, LayerNorm computes the mean \\( \\mu \\) and variance \\( \\sigma^2 \\) of the features in that data point. This is done individually for each data point and is given by:\n",
        "   \\[\n",
        "   \\mu_i = \\frac{1}{D} \\sum_{j=1}^{D} x_{ij}\n",
        "   \\]\n",
        "   \\[\n",
        "   \\sigma^2_i = \\frac{1}{D} \\sum_{j=1}^{D} (x_{ij} - \\mu_i)^2\n",
        "   \\]\n",
        "   where \\( i \\) indexes the data points and \\( j \\) indexes the features.\n",
        "\n",
        "3. **Scaling and Shifting:**\n",
        "   After calculating the mean and variance for each data point, LayerNorm normalizes the input by subtracting the mean and dividing by the standard deviation (with a small epsilon added for numerical stability):\n",
        "   \\[\n",
        "   \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_i}{\\sqrt{\\sigma^2_i + \\epsilon}}\n",
        "   \\]\n",
        "   Then, two learnable parameters, gamma \\( \\gamma \\) and beta \\( \\beta \\), are introduced to scale and shift the normalized output:\n",
        "   \\[\n",
        "   y_{ij} = \\gamma_j \\hat{x}_{ij} + \\beta_j\n",
        "   \\]\n",
        "   This ensures that the model can learn to revert back to the original activations if necessary.\n",
        "\n",
        "### Key Differences Between LayerNorm and BatchNorm:\n",
        "- **BatchNorm** normalizes across the batch dimension and is dependent on the batch size, which can cause problems for small batch sizes.\n",
        "- **LayerNorm** normalizes across the features, making it independent of the batch size and more suitable for sequential tasks or tasks with varying batch sizes.\n",
        "\n",
        "### LayerNorm in PyTorch:\n",
        "In PyTorch, you can use `torch.nn.LayerNorm` to apply Layer Normalization.\n",
        "\n",
        "Hereâ€™s an example:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example input tensor with batch size 3 and feature size 4\n",
        "x = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
        "                  [2.0, 4.0, 6.0, 8.0],\n",
        "                  [3.0, 6.0, 9.0, 12.0]])\n",
        "\n",
        "# Define a LayerNorm with feature size 4\n",
        "layer_norm = nn.LayerNorm(4)\n",
        "\n",
        "# Apply LayerNorm\n",
        "output = layer_norm(x)\n",
        "\n",
        "# Print the input and output\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after LayerNorm:\\n\", output)\n",
        "```\n",
        "\n",
        "### Explanation of the code:\n",
        "- We define an input tensor `x` with shape `[3, 4]` (3 data points, 4 features each).\n",
        "- We initialize `nn.LayerNorm` with the size of the feature dimension, which is `4` in this case.\n",
        "- We apply LayerNorm to the input tensor and print both the original input and the normalized output.\n",
        "\n",
        "Layer Normalization is particularly useful in transformer-based architectures like BERT or GPT, where the focus is on normalizing each individual sequence or token representation rather than the whole batch.\n",
        "\n"
      ],
      "metadata": {
        "id": "fYDD7S-MYGFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example input tensor with batch size 3 and feature size 4\n",
        "x = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
        "                  [2.0, 4.0, 6.0, 8.0],\n",
        "                  [3.0, 6.0, 9.0, 12.0]])\n",
        "\n",
        "# Define a LayerNorm with feature size 4\n",
        "layer_norm = nn.LayerNorm(4)\n",
        "\n",
        "# Apply LayerNorm\n",
        "outputLayer = layer_norm(x)\n",
        "\n",
        "# Print the input and output\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after LayerNorm:\\n\", outputLayer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGdJCrdiYUAM",
        "outputId": "df3af90d-d741-485f-8506-f80dc1fa1b81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[ 1.,  2.,  3.,  4.],\n",
            "        [ 2.,  4.,  6.,  8.],\n",
            "        [ 3.,  6.,  9., 12.]])\n",
            "Output after LayerNorm:\n",
            " tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
            "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
            "        [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LayerNorm vs BatchNorm\n",
        "\n",
        " Unlike LayerNorm, BatchNorm normalizes across the batch dimension. It calculates the mean and variance for each feature across the entire batch and then normalizes those feature values.\n",
        "\n",
        "\n",
        "\n",
        "### Batch Normalization Example in PyTorch:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example input tensor with batch size 3 and feature size 4\n",
        "x = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
        "                  [2.0, 4.0, 6.0, 8.0],\n",
        "                  [3.0, 6.0, 9.0, 12.0]])\n",
        "\n",
        "# Define a BatchNorm layer with feature size 4 (normalize over the feature dimension)\n",
        "batch_norm = nn.BatchNorm1d(4)\n",
        "\n",
        "# Apply BatchNorm\n",
        "output = batch_norm(x)\n",
        "\n",
        "# Print the input and output\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after BatchNorm:\\n\", output)\n",
        "```\n",
        "\n",
        "### Explanation of the Code:\n",
        "- The input tensor `x` has a shape of `[3, 4]`, representing 3 samples (batch size of 3), each with 4 features.\n",
        "- We use `nn.BatchNorm1d(4)` for normalizing the feature dimension (4 in this case). The \"1d\" here signifies that this is 1D data, which fits our use case.\n",
        "- The Batch Normalization layer will calculate the mean and variance for each feature across the entire batch and normalize the values accordingly.\n",
        "- The output tensor will have normalized values based on these calculations.\n",
        "\n",
        "### Key Difference from LayerNorm:\n",
        "- **BatchNorm** computes the mean and variance for each feature across the entire batch, which depends on the batch size. It works well when the batch size is reasonably large.\n",
        "- **LayerNorm**, on the other hand, normalizes across the features within each data point, making it independent of the batch size.\n",
        "\n"
      ],
      "metadata": {
        "id": "7D9TtTbjZnDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example input tensor with batch size 3 and feature size 4\n",
        "x = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
        "                  [2.0, 4.0, 6.0, 8.0],\n",
        "                  [3.0, 6.0, 9.0, 12.0]])\n",
        "\n",
        "# Define a BatchNorm layer with feature size 4 (normalize over the feature dimension)\n",
        "batch_norm = nn.BatchNorm1d(4)\n",
        "\n",
        "# Apply BatchNorm\n",
        "outputBatch = batch_norm(x)\n",
        "\n",
        "# Print the input and output\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after BatchNorm:\\n\", outputBatch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt66zdXMZ--w",
        "outputId": "83f2bbef-d9be-4074-c01e-5ced7fc87cf7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[ 1.,  2.,  3.,  4.],\n",
            "        [ 2.,  4.,  6.,  8.],\n",
            "        [ 3.,  6.,  9., 12.]])\n",
            "Output after BatchNorm:\n",
            " tensor([[-1.2247, -1.2247, -1.2247, -1.2247],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 1.2247,  1.2247,  1.2247,  1.2247]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\\n\", x)\n",
        "print(\"\\nOutput after BatchNorm:\\n\\n\", outputLayer)\n",
        "print(\"\\nOutput after BatchNorm:\\n\\n\", outputBatch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWh1FDJ3aIY5",
        "outputId": "088ee20e-3097-4581-c442-cbebea426fa2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "\n",
            " tensor([[ 1.,  2.,  3.,  4.],\n",
            "        [ 2.,  4.,  6.,  8.],\n",
            "        [ 3.,  6.,  9., 12.]])\n",
            "\n",
            "Output after BatchNorm:\n",
            "\n",
            " tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
            "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
            "        [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Output after BatchNorm:\n",
            "\n",
            " tensor([[-1.2247, -1.2247, -1.2247, -1.2247],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 1.2247,  1.2247,  1.2247,  1.2247]],\n",
            "       grad_fn=<NativeBatchNormBackward0>)\n"
          ]
        }
      ]
    }
  ]
}