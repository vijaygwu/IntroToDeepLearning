{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbqJtIClqjvtEiuuu24TGH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/IntroToDeepLearning/blob/main/AutoGradPyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Differentiation (Autograd) in PyTorch: A Key to Deep Learning\n",
        "\n",
        "**Summary:** Autograd in PyTorch is a powerful tool that automates the gradient computation process, making it significantly easier to train complex deep learning models. Automatic differentiation, often referred to as \"autograd\" in the PyTorch world, is the cornerstone of how neural networks are trained efficiently. It's a technique for automatically calculating the gradients (derivatives) of a function with respect to its inputs. In deep learning, this function is typically the loss function of your model, and the inputs are the model's parameters (weights and biases).\n",
        "\n",
        "****\n",
        "\n",
        "**How it Works**\n",
        "\n",
        "1. **Dynamic Computational Graph:**\n",
        "   * **Building the Graph:** As you perform operations on PyTorch tensors, PyTorch constructs a dynamic computational graph behind the scenes. Each node in this graph represents an operation (e.g., addition, matrix multiplication), and the edges represent the flow of data (tensors) between operations.\n",
        "   * **Tracking Operations:** PyTorch keeps track of every operation that acts on a tensor, including the input tensors and the function used. This creates a chain of dependencies within the graph.\n",
        "\n",
        "2. **Forward Pass:**\n",
        "   * **Computation:**  During the forward pass, you feed your input data through the model, and PyTorch executes the operations in the graph, calculating the output (predictions) and storing intermediate results.\n",
        "\n",
        "3. **Backward Pass (Backpropagation):**\n",
        "   * **Gradient Calculation:**  Once you have the loss (a measure of how well your model is performing), you call `.backward()` on the loss tensor. This triggers the backward pass through the computational graph.\n",
        "   * **Chain Rule:**  PyTorch applies the chain rule of calculus to automatically compute the gradients of the loss with respect to each parameter in the model. It does this by traversing the graph in reverse, using the stored intermediate results and the derivatives of each operation.\n",
        "   * **Gradient Accumulation:**  The gradients for each parameter are accumulated into the `.grad` attribute of the corresponding tensor.\n",
        "\n",
        "4. **Parameter Update:**\n",
        "   * **Optimizer:**  An optimizer (e.g., `torch.optim.SGD`, `torch.optim.Adam`) uses the computed gradients to update the model's parameters, aiming to minimize the loss.\n",
        "\n",
        "**Key Advantages:**\n",
        "\n",
        "* **Efficiency:** Autograd eliminates the need to manually derive and implement complex gradient calculations, making deep learning model development much faster and less error-prone.\n",
        "* **Flexibility:**  The dynamic graph construction allows for easy experimentation with different model architectures and control flow within your code.\n",
        "* **GPU Acceleration:**  Since PyTorch tensors can reside on GPUs, the gradient computations can also be performed on the GPU, leading to significant speedups.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vpED6fqLsGCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1**"
      ],
      "metadata": {
        "id": "StyqfnaVuYQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create tensors and require gradients\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "w = torch.tensor([3.0], requires_grad=True)\n",
        "b = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "# Define a simple computation\n",
        "y = w * x + b\n",
        "\n",
        "# Compute gradients\n",
        "y.backward()\n",
        "\n",
        "# Access the computed gradients\n",
        "print(x.grad)  # Output: tensor([3.])\n",
        "print(w.grad)  # Output: tensor([2.])\n",
        "print(b.grad)  # Output: tensor([1.])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqNBcQuUsKjg",
        "outputId": "33b6a159-45fa-41c1-f161-3d72d762e4b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.])\n",
            "tensor([2.])\n",
            "tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "1. We create tensors `x`, `w`, and `b` and set `requires_grad=True` to tell PyTorch to track operations on them.\n",
        "2. We perform a computation (`y = w * x + b`), building a computational graph.\n",
        "3. Calling `y.backward()` triggers backpropagation, calculating the gradients of `y` with respect to `x`, `w`, and `b`.\n",
        "4. The gradients are stored in the `.grad` attribute of each tensor."
      ],
      "metadata": {
        "id": "zdPk7t5kskzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2**"
      ],
      "metadata": {
        "id": "_CwT4oXvuc-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=5, out_features=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=3, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "model = MyNet()\n",
        "\n",
        "# Input data and target (ground truth)\n",
        "input_data = torch.randn(1, 5)\n",
        "target = torch.randn(1, 1)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_data)\n",
        "loss = criterion(output, target)\n",
        "\n",
        "# Backward pass (autograd in action!)\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "# Print gradients for each parameter\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"Gradient for {name}: {param.grad}\")\n",
        "\n",
        "# Parameter update\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Updated Parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.data}\")\n",
        "\n",
        "print(\"Loss:\", loss.item())\n",
        "print(\"Output:\", output.item())\n",
        "print(\"Target:\", target.item())\n",
        "print(\"Model Output:\", model(input_data).item())\n",
        "print(\"Model Output:\", model(input_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkI8KI6It47D",
        "outputId": "10093060-8f16-496c-f497-089aff43debf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient for fc1.weight: tensor([[ 0.2684,  0.6162,  0.2490,  0.5346, -0.2217],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "        [-0.0974, -0.2235, -0.0903, -0.1939,  0.0804]])\n",
            "Gradient for fc1.bias: tensor([ 0.5501,  0.0000, -0.1995])\n",
            "Gradient for fc2.weight: tensor([[1.1551, 0.0000, 0.8438]])\n",
            "Gradient for fc2.bias: tensor([1.2540])\n",
            "Updated Parameters:\n",
            "fc1.weight: tensor([[ 0.1212,  0.2940,  0.1535,  0.4415, -0.0450],\n",
            "        [-0.3487, -0.1964, -0.2726,  0.0353,  0.2814],\n",
            "        [ 0.2427, -0.0500,  0.4210, -0.1063, -0.2250]])\n",
            "fc1.bias: tensor([-0.0050,  0.0597,  0.4400])\n",
            "fc2.weight: tensor([[ 0.4271,  0.2673, -0.1675]])\n",
            "fc2.bias: tensor([-0.0380])\n",
            "Loss: 0.3931587040424347\n",
            "Output: 0.2715688645839691\n",
            "Target: -0.35545486211776733\n",
            "Model Output: 0.23250102996826172\n",
            "Model Output: tensor([[0.2325]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "1. **Neural Network Definition:** We define a simple neural network with two fully connected layers and a ReLU activation function in between.\n",
        "\n",
        "2. **Input and Target:** We create random input data and a target value to simulate a training example.\n",
        "\n",
        "3. **Loss and Optimizer:**\n",
        "   * **Loss Function:** We use Mean Squared Error (MSE) loss to measure the discrepancy between the model's output and the target.\n",
        "   * **Optimizer:** We choose Stochastic Gradient Descent (SGD) to update the model's parameters.\n",
        "\n",
        "4. **Forward Pass:**\n",
        "   * `output = model(input_data)`: This line feeds the input data through the network, executing the `forward` method of `MyNet`.\n",
        "   * `loss = criterion(output, target)`: Calculates the loss by comparing the model's output to the target.\n",
        "   * **Dynamic Graph:** During this forward pass, PyTorch constructs a dynamic computational graph, keeping track of all operations performed on the tensors.\n",
        "\n",
        "5. **Backward Pass (Autograd's Magic):**\n",
        "   * `loss.backward()`: This single line initiates the backward pass (backpropagation) through the computational graph.\n",
        "   * **Gradient Computation:** PyTorch automatically applies the chain rule to calculate the gradients of the loss with respect to each parameter (weights and biases) in the network.\n",
        "   * **Gradient Storage:** The computed gradients are stored in the `.grad` attribute of each parameter tensor.\n",
        "\n",
        "6. **Parameter Update:**\n",
        "   * `optimizer.step()`:  The optimizer uses the calculated gradients to update the model's parameters, nudging them in a direction that aims to reduce the loss.\n",
        "\n",
        "**Key Points:**\n",
        "\n",
        "* **No Manual Gradient Calculation:** You don't need to explicitly derive or implement the gradient calculations for each operation in your network. PyTorch handles it automatically.\n",
        "* **Dynamic Graph Construction:** The graph is built as your code executes, offering flexibility for complex models and control flow.\n",
        "* **Efficiency:** The automatic differentiation process is highly optimized, especially when leveraging GPUs.\n",
        "\n",
        "**In Conclusion:**\n",
        "\n",
        "This example showcases how PyTorch's automatic differentiation simplifies the training process of neural networks. By automatically computing gradients, PyTorch empowers you to focus on designing and experimenting with models without the burden of manual gradient calculations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VDAky56QtOtT"
      }
    }
  ]
}